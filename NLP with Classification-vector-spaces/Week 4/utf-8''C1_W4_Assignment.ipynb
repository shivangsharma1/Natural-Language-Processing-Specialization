{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Naive Machine Translation and LSH\n",
    "\n",
    "You will now implement your first machine translation system and then you\n",
    "will see how locality sensitive hashing works. Let's get started by importing\n",
    "the required functions!\n",
    "\n",
    "If you are running this notebook in your local computer, don't forget to\n",
    "download the twitter samples and stopwords from nltk.\n",
    "\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The `Exercise xx` numbers in this assignment **_are inconsistent_** with the `UNQ_Cx` numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This assignment covers the folowing topics:\n",
    "\n",
    "- [1. The word embeddings data for English and French words](#1)\n",
    "  - [1.1 Generate embedding and transform matrices](#1-1)\n",
    "      - [Exercise 1](#ex-01)\n",
    "- [2. Translations](#2)\n",
    "  - [2.1 Translation as linear transformation of embeddings](#2-1)\n",
    "      - [Exercise 2](#ex-02)  \n",
    "      - [Exercise 3](#ex-03)  \n",
    "      - [Exercise 4](#ex-04)        \n",
    "  - [2.2 Testing the translation](#2-2)\n",
    "      - [Exercise 5](#ex-05)\n",
    "      - [Exercise 6](#ex-06)      \n",
    "- [3. LSH and document search](#3)\n",
    "  - [3.1 Getting the document embeddings](#3-1)\n",
    "      - [Exercise 7](#ex-07)\n",
    "      - [Exercise 8](#ex-08)      \n",
    "  - [3.2 Looking up the tweets](#3-2)\n",
    "  - [3.3 Finding the most similar tweets with LSH](#3-3)\n",
    "  - [3.4 Getting the hash number for a vector](#3-4)\n",
    "      - [Exercise 9](#ex-09)  \n",
    "  - [3.5 Creating a hash table](#3-5)\n",
    "      - [Exercise 10](#ex-10)  \n",
    "  - [3.6 Creating all hash tables](#3-6)\n",
    "      - [Exercise 11](#ex-11)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from utils import (cosine_similarity, get_dict,\n",
    "                   process_tweet)\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "# 1. The word embeddings data for English and French words\n",
    "\n",
    "Write a program that translates English to French.\n",
    "\n",
    "## The data\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes, and the French\n",
    "embeddings are about 629 megabytes. To prevent the Coursera workspace from\n",
    "crashing, we've extracted a subset of the embeddings for the words that you'll\n",
    "use in this assignment.\n",
    "\n",
    "If you want to run this on your local computer and use the full dataset,\n",
    "you can download the\n",
    "* English embeddings from Google code archive word2vec\n",
    "[look for GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
    "    * You'll need to unzip the file first.\n",
    "* and the French embeddings from\n",
    "[cross_lingual_text_classification](https://github.com/vjstark/crosslingual_text_classification).\n",
    "    * in the terminal, type (in one line)\n",
    "    `curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec`\n",
    "\n",
    "Then copy-paste the code below and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Use this code to download and process the full dataset on your local computer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n",
    "\n",
    "\n",
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the english to french test dictionary is', len(en_fr_train))\n",
    "\n",
    "english_set = set(en_embeddings.vocab)\n",
    "french_set = set(fr_embeddings.vocab)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "french_words = set(en_fr_train.values())\n",
    "\n",
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "for en_word in en_fr_test.keys():\n",
    "    fr_word = en_fr_test[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\n",
    "pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The subset of data\n",
    "\n",
    "To do the assignment on the Coursera workspace, we'll use the subset of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the data\n",
    "\n",
    "* en_embeddings_subset: the key is an English word, and the vaule is a\n",
    "300 dimensional array, which is the embedding for that word.\n",
    "```\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "```\n",
    "\n",
    "* fr_embeddings_subset: the key is an French word, and the vaule is a 300\n",
    "dimensional array, which is the embedding for that word.\n",
    "```\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load two dictionaries mapping the English to French words\n",
    "* A training dictionary\n",
    "* and a testing dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the English French dictionary\n",
    "\n",
    "* `en_fr_train` is a dictionary where the key is the English word and the value\n",
    "is the French translation of that English word.\n",
    "```\n",
    "{'the': 'la',\n",
    " 'and': 'et',\n",
    " 'was': 'Ã©tait',\n",
    " 'for': 'pour',\n",
    "```\n",
    "\n",
    "* `en_fr_test` is similar to `en_fr_train`, but is a test set.  We won't look at it\n",
    "until we get to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-1\"></a>\n",
    "\n",
    "## 1.1 Generate embedding and transform matrices\n",
    "\n",
    "<a name=\"ex-01\"></a>\n",
    "#### Exercise 01: Translating English dictionary to French by using embeddings\n",
    "\n",
    "You will now implement a function `get_matrices`, which takes the loaded data\n",
    "and returns matrices `X` and `Y`.\n",
    "\n",
    "Inputs:\n",
    "- `en_fr` : English to French dictionary\n",
    "- `en_embeddings` : English to embeddings dictionary\n",
    "- `fr_embeddings` : French to embeddings dictionary\n",
    "\n",
    "Returns:\n",
    "- Matrix `X` and matrix `Y`, where each row in X is the word embedding for an\n",
    "english word, and the same row in Y is the word embedding for the French\n",
    "version of that English word.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\">\n",
    "<img src='X_to_Y.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:800px;height:200px;\" /> Figure 2 </div>\n",
    "\n",
    "Use the `en_fr` dictionary to ensure that the ith row in the `X` matrix\n",
    "corresponds to the ith row in the `Y` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Complete the function `get_matrices()`:\n",
    "* Iterate over English words in `en_fr` dictionary.\n",
    "* Check if the word have both English and French embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "    <p>\n",
    "        <ul>\n",
    "            <li><a href=\"https://realpython.com/python-sets/#set-size-and-membership\" >Sets</a> are useful data structures that can be used to check if an item is a member of a group.</li>\n",
    "            <li>You can get words which are embedded into the language by using <a href=\"https://www.w3schools.com/python/ref_dictionary_keys.asp\"> keys</a> method.</li>\n",
    "            <li>Keep vectors in `X` and `Y` sorted in list. You can use <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ma.vstack.html\"> np.vstack()</a> to merge them into the numpy matrix. </li>\n",
    "            <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html\">numpy.vstack</a> stacks the items in a list as rows in a matrix.</li>\n",
    "        </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.vstack(Y_l)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use function `get_matrices()` to obtain sets `X_train` and `Y_train`\n",
    "of English and French word embeddings into the corresponding vector space models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# getting the training set:\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "# 2. Translations\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='e_to_f.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:200px;\" /> Figure 1 </div>\n",
    "\n",
    "Write a program that translates English words to French words using word embeddings and vector space models. \n",
    "\n",
    "<a name=\"2-1\"></a>\n",
    "## 2.1 Translation as linear transformation of embeddings\n",
    "\n",
    "Given dictionaries of English and French word embeddings you will create a transformation matrix `R`\n",
    "* Given an English word embedding, $\\mathbf{e}$, you can multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
    "    * Both $\\mathbf{e}$ and $\\mathbf{f}$ are [row vectors](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
    "* You can then compute the nearest neighbors to `f` in the french embeddings and recommend the word that is most similar to the transformed word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing translation as the minimization problem\n",
    "\n",
    "Find a matrix `R` that minimizes the following equation. \n",
    "\n",
    "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
    "\n",
    "### Frobenius norm\n",
    "\n",
    "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual loss function\n",
    "In the real world applications, the Frobenius norm loss:\n",
    "\n",
    "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
    "\n",
    "is often replaced by it's squared value divided by $m$:\n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
    "\n",
    "* The same R is found when using this loss function versus the original Frobenius norm.\n",
    "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.\n",
    "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
    "    * The loss for all training set increases with more words (training examples),\n",
    "    so taking the average helps us to track the average loss regardless of the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Optional] Detailed explanation why we use norm squared instead of the norm:\n",
    "<details>\n",
    "<summary>\n",
    "    Click for optional details\n",
    "</summary>\n",
    "    <p>\n",
    "        <ul>\n",
    "            <li>The norm is always nonnegative (we're summing up absolute values), and so is the square. \n",
    "            <li> When we take the square of all non-negative (positive or zero) numbers, the order of the data is preserved.  \n",
    "            <li> For example, if 3 > 2, 3^2 > 2^2\n",
    "            <li> Using the norm or squared norm in gradient descent results in the same <i>location</i> of the minimum.\n",
    "            <li> Squaring cancels the square root in the Frobenius norm formula. Because of the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\"> chain rule</a>, we would have to do more calculations if we had a square root in our expression for summation.\n",
    "            <li> Dividing the function value by the positive number doesn't change the optimum of the function, for the same reason as described above.\n",
    "            <li> We're interested in transforming English embedding into the French. Thus, it is more important to measure average loss per embedding than the loss for the entire dictionary (which increases as the number of words in the dictionary increases).\n",
    "        </ul>\n",
    "    </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-02\"></a>\n",
    "\n",
    "### Exercise 02: Implementing translation mechanism described in this section.\n",
    "\n",
    "#### Step 1: Computing the loss\n",
    "* The loss function will be squared Frobenoius norm of the difference between\n",
    "matrix and its approximation, divided by the number of training examples $m$.\n",
    "* Its formula is:\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions: complete the `compute_loss()` function\n",
    "\n",
    "* Compute the approximation of `Y` by matrix multiplying `X` and `R`\n",
    "* Compute difference `XR - Y`\n",
    "* Compute the squared Frobenius norm of the difference and divide it by $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "   <li> Useful functions:\n",
    "       <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html\">Numpy dot </a>,\n",
    "       <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\">Numpy sum</a>,\n",
    "       <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.square.html\">Numpy square</a>,\n",
    "       <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html\">Numpy norm</a>\n",
    "    </li>\n",
    "   <li> Be careful about which operation is elementwise and which operation is a matrix multiplication.</li>\n",
    "   <li> Try to use matrix operations instead of the numpy norm function.  If you choose to use norm function, take care of extra arguments and that it's returning loss squared, and not the loss itself.</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X, R) - Y \n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared / m\n",
    "    ### END CODE HERE ###\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-03\"></a>\n",
    "\n",
    "### Exercise 03\n",
    "\n",
    "### Step 2: Computing the gradient of loss in respect to transform matrix R\n",
    "\n",
    "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
    "* The gradient is a matrix that encodes how much a small change in `R`\n",
    "affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease `R`\n",
    "to minimize the loss.\n",
    "* $m$ is the number of training examples (number of rows in $X$).\n",
    "* The formula for the gradient of the loss function $ð¿(ð,ð,ð)$ is:\n",
    "\n",
    "$$\\frac{d}{dR}ð¿(ð,ð,ð)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "\n",
    "**Instructions**: Complete the `compute_gradient` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "    <ul>\n",
    "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.T.html\" > Transposing in numpy </a></li>\n",
    "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html\" > Finding out the dimensions</a> of matrices in numpy </li>\n",
    "    <li>Remember to use numpy.dot for matrix multiplication </li>\n",
    "    </ul>\n",
    "</p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = (np.dot(X.T , np.dot(X, R) - Y)) * (2/m)\n",
    "    ### END CODE HERE ###\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Finding the optimal R with gradient descent algorithm\n",
    "\n",
    "#### Gradient descent\n",
    "\n",
    "[Gradient descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) is an iterative algorithm which is used in searching for the optimum of the function. \n",
    "* Earlier, we've mentioned that the gradient of the loss with respect to the matrix encodes how much a tiny change in some coordinate of that matrix affect the change of loss function.\n",
    "* Gradient descent uses that information to iteratively change matrix `R` until we reach a point where the loss is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with a fixed number of iterations\n",
    "\n",
    "Most of the time we iterate for a fixed number of training steps rather than iterating until the loss falls below a threshold.\n",
    "\n",
    "##### OPTIONAL: explanation for fixed number of iterations\n",
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>click here for detailed discussion</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> You cannot rely on training loss getting low -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as \"early stopping\" -- until the validation accuracy starts to go down, which is a sign of over-fitting.\n",
    "    </li>\n",
    "    <li>\n",
    "    Why not always do \"early stopping\"? Well, mostly because well-regularized models on larger data-sets never stop improving. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?\n",
    "    </li>\n",
    "    <li>Stopping after a certain number of steps has the advantage that you know how long your training will take - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. Another advantage is that you can fix your learning rate schedule -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training.\n",
    "    </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "1. Calculate gradient $g$ of the loss with respect to the matrix $R$.\n",
    "2. Update $R$ with the formula:\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, which is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "\n",
    "* The learning rate or \"step size\" $\\alpha$ is a coefficient which decides how much we want to change $R$ in each step.\n",
    "* If we change $R$ too much, we could skip the optimum by taking too large of a step.\n",
    "* If we make only small changes to $R$, we will need many steps to reach the optimum.\n",
    "* Learning rate $\\alpha$ is used to control those changes.\n",
    "* Values of $\\alpha$ are chosen depending on the problem, and we'll use `learning_rate`$=0.0003$ as the default value for our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-04\"></a>\n",
    "\n",
    "### Exercise 04\n",
    "\n",
    "#### Instructions: Implement `align_embeddings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the 'compute_gradient()' function to get the gradient in each step</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "        ### END CODE HERE ###\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Testing your implementation.\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "loss at iteration 0 is: 3.7242\n",
    "loss at iteration 25 is: 3.6283\n",
    "loss at iteration 50 is: 3.5350\n",
    "loss at iteration 75 is: 3.4442\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate transformation matrix R\n",
    "\n",
    "Using those the training set, find the transformation matrix $\\mathbf{R}$ by calling the function `align_embeddings()`.\n",
    "\n",
    "**NOTE:** The code cell below will take a few minutes to fully execute (~3 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```\n",
    "loss at iteration 0 is: 963.0146\n",
    "loss at iteration 25 is: 97.8292\n",
    "loss at iteration 50 is: 26.8329\n",
    "loss at iteration 75 is: 9.7893\n",
    "loss at iteration 100 is: 4.3776\n",
    "loss at iteration 125 is: 2.3281\n",
    "loss at iteration 150 is: 1.4480\n",
    "loss at iteration 175 is: 1.0338\n",
    "loss at iteration 200 is: 0.8251\n",
    "loss at iteration 225 is: 0.7145\n",
    "loss at iteration 250 is: 0.6534\n",
    "loss at iteration 275 is: 0.6185\n",
    "loss at iteration 300 is: 0.5981\n",
    "loss at iteration 325 is: 0.5858\n",
    "loss at iteration 350 is: 0.5782\n",
    "loss at iteration 375 is: 0.5735\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2-2\"></a>\n",
    "\n",
    "## 2.2 Testing the translation\n",
    "\n",
    "### k-Nearest neighbors algorithm\n",
    "\n",
    "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
    "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. \n",
    "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
    "\n",
    "### Searching for the translation embedding\n",
    "Since we're approximating the translation function from English to French embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a French word when we transform embedding $\\mathbf{e}$ of some particular English word into the French embedding space. \n",
    "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
    "The formula is \n",
    "\n",
    "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
    "\n",
    "* $\\cos(u,v)$ = $1$ when $u$ and $v$ lie on the same line and have the same direction.\n",
    "* $\\cos(u,v)$ is $-1$ when they have exactly opposite directions.\n",
    "* $\\cos(u,v)$ is $0$ when the vectors are orthogonal (perpendicular) to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Distance and similarity are pretty much opposite things.\n",
    "* We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric. \n",
    "* When the cosine similarity increases (towards $1$), the \"distance\" between the two vectors decreases (towards $0$). \n",
    "* We can define the cosine distance between $u$ and $v$ as\n",
    "$$d_{\\text{cos}}(u,v)=1-\\cos(u,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-05\"></a>\n",
    "\n",
    "**Exercise 05**: Complete the function `nearest_neighbor()`\n",
    "\n",
    "Inputs:\n",
    "* Vector `v`,\n",
    "* A set of possible nearest neighbors `candidates`\n",
    "* `k` nearest neighbors to find.\n",
    "* The distance metric should be based on cosine similarity.\n",
    "* `cosine_similarity` function is already implemented and imported for you. It's arguments are two vectors and it returns the cosine of the angle between them.\n",
    "* Iterate over rows in `candidates`, and save the result of similarities between current row and vector `v` in a python list. Take care that similarities are in the same order as row vectors of `candidates`.\n",
    "* Now you can use [numpy argsort]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort) to sort the indices for the rows of `candidates`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> numpy.argsort sorts values from most negative to most positive (smallest to largest) </li>\n",
    "    <li> The candidates that are nearest to 'v' should have the highest cosine similarity </li>\n",
    "    <li> To get the last element of a list 'tmp', the notation is tmp[-1:] </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    ### END CODE HERE ###\n",
    "    return k_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4 0 2]\n",
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Test your implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "`[[9 9 9]\n",
    " [1 0 5]\n",
    " [2 0 1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your translation and compute its accuracy\n",
    "\n",
    "<a name=\"ex-06\"></a>\n",
    "**Exercise 06**:\n",
    "Complete the function `test_vocabulary` which takes in English\n",
    "embedding matrix $X$, French embedding matrix $Y$ and the $R$\n",
    "matrix and returns the accuracy of translations from $X$ to $Y$ by $R$.\n",
    "\n",
    "* Iterate over transformed English word embeddings and check if the\n",
    "closest French word vector belongs to French word that is the actual\n",
    "translation.\n",
    "* Obtain an index of the closest French embedding by using\n",
    "`nearest_neighbor` (with argument `k=1`), and compare it to the index\n",
    "of the English embedding you have just transformed.\n",
    "* Keep track of the number of times you get the correct translation.\n",
    "* Calculate accuracy as $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X , R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct / pred.shape[0]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how is your translation mechanism working on the unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 194   40  421 ... 1198  520    0]\n",
      "[413 611 369 ...  94 331  47]\n",
      "[1005  584 1277 ...   42 1242    2]\n",
      "[ 308  418 1073 ... 1249 1110  559]\n",
      "[ 308 1365  122 ...   24  777 1367]\n",
      "[1392  506 1209 ...  550 1359   55]\n",
      "[ 551 1282 1185 ...  272  218    6]\n",
      "[ 829  525  418 ...  780 1394 1132]\n",
      "[ 636 1100 1364 ...  715 1155    8]\n",
      "[1249 1363  521 ...  137 1125    9]\n",
      "[  70  952   61 ... 1012  548   10]\n",
      "[ 994  413   70 ... 1296  674   11]\n",
      "[1413  550  925 ...  533   89   12]\n",
      "[ 392  566 1347 ...  345   34   13]\n",
      "[  84 1323   65 ...  548  140   14]\n",
      "[ 187  923 1431 ... 1090  116  488]\n",
      "[ 316  291 1151 ... 1157  274   16]\n",
      "[1276  554  564 ...  160   23 1367]\n",
      "[ 231 1012 1115 ...  504   18 1396]\n",
      "[ 195 1007  457 ...  576 1199   19]\n",
      "[1417  874  970 ...  119  504   20]\n",
      "[ 242  735  730 ...  991 1342  644]\n",
      "[ 792 1015  891 ...  294 1188   22]\n",
      "[  84  466  145 ...  377 1206   23]\n",
      "[ 194 1105  864 ...  670  629   24]\n",
      "[ 376  602  945 ...   25  182 1312]\n",
      "[ 398  241 1127 ... 1016   26  502]\n",
      "[188 835 613 ... 859 740  27]\n",
      "[1347 1038   84 ...  582  426   28]\n",
      "[1341  596  584 ...  324   34   29]\n",
      "[1353  581 1221 ... 1047  772 1391]\n",
      "[1365  525  369 ...  550  777   24]\n",
      "[ 497 1163 1407 ...  849 1090  116]\n",
      "[1185  779  571 ...  688  146   33]\n",
      "[ 989  952 1071 ...   29  107   34]\n",
      "[ 308  890  951 ... 1110  559   35]\n",
      "[ 363  380 1074 ...  314  896   36]\n",
      "[ 128  528  237 ...  198 1341   37]\n",
      "[1417   90  146 ... 1266  639   38]\n",
      "[671 110 622 ... 582 426  39]\n",
      "[ 339  407  743 ... 1238 1284   40]\n",
      "[  93  922 1045 ... 1016   45   41]\n",
      "[1430  382  218 ... 1274   42 1128]\n",
      "[ 491 1100  818 ... 1343  210   43]\n",
      "[ 308  369  431 ...  346 1091  624]\n",
      "[1352  376 1080 ...   41 1016   45]\n",
      "[743 441  70 ... 539 760  46]\n",
      "[ 228  679 1014 ...  371 1068   47]\n",
      "[751  52 772 ... 227 823 944]\n",
      "[ 706 1071  554 ...  481  918   49]\n",
      "[ 376  765  397 ... 1194   10   50]\n",
      "[1094  695 1014 ...  238   51  622]\n",
      "[ 128  177 1291 ...  146   86   52]\n",
      "[1247  211  437 ...  447  767  695]\n",
      "[128 748  18 ... 354 887 576]\n",
      "[141 799 768 ...  55 531 964]\n",
      "[ 122 1285  422 ...  404 1113   56]\n",
      "[1207 1422  139 ... 1348 1232   57]\n",
      "[1046 1214   44 ...  782   58 1341]\n",
      "[ 315 1332  116 ... 1016   41   59]\n",
      "[ 919 1096  303 ...  695  377   60]\n",
      "[ 257 1214  831 ...  438 1092   61]\n",
      "[ 122 1021  802 ...  516 1367  410]\n",
      "[ 360   84   18 ... 1204  409   63]\n",
      "[1323  211  423 ...  548  648  744]\n",
      "[ 277  843 1303 ...  962   65   37]\n",
      "[1370 1186   85 ...  846 1195 1357]\n",
      "[ 316  291 1246 ...  726  642   67]\n",
      "[  16  611 1142 ...  572  116 1090]\n",
      "[1019  518  555 ... 1046 1104   21]\n",
      "[ 827  122 1106 ... 1367 1121   24]\n",
      "[ 611 1100  679 ...  200 1066   71]\n",
      "[ 670 1110  559 ... 1341  202   72]\n",
      "[ 310  917 1012 ... 1148 1054  944]\n",
      "[557  49 387 ... 933 756 279]\n",
      "[ 495  110  299 ...  667 1313  979]\n",
      "[ 420  601  892 ...   76 1206  389]\n",
      "[ 261 1074  603 ... 1250 1389  955]\n",
      "[ 501  335  153 ... 1378  384   78]\n",
      "[ 800 1086 1014 ...  852  658   79]\n",
      "[1279  690 1247 ...  411  909 1270]\n",
      "[1042 1139  840 ...  485 1416  909]\n",
      "[ 843  474  689 ...  378 1112  123]\n",
      "[  81 1085 1149 ... 1296  481   83]\n",
      "[1009  461  287 ... 1411  232  443]\n",
      "[ 454  298  205 ...   85 1405   91]\n",
      "[ 432 1323 1309 ... 1425   86 1064]\n",
      "[1276  404  145 ...   94  688 1099]\n",
      "[1373  486  153 ...  249 1378   88]\n",
      "[ 451 1172  608 ... 1092  893 1410]\n",
      "[ 262 1074  914 ...  370  813   90]\n",
      "[998 783 750 ... 964 981  91]\n",
      "[178 176 897 ... 896 297 697]\n",
      "[1310  418  432 ...  631  674   94]\n",
      "[1276 1241  961 ... 1432  252   94]\n",
      "[ 363   84 1323 ...   95 1056  794]\n",
      "[1071  404  173 ...  213 1115 1017]\n",
      "[ 755  254  300 ... 1125  296   97]\n",
      "[989 735 646 ... 609 107  98]\n",
      "[ 942  537  244 ... 1056  794   95]\n",
      "[ 998  505 1374 ...  668 1307  100]\n",
      "[ 432   50  544 ... 1173  220  101]\n",
      "[167 803 949 ... 702 102 368]\n",
      "[245 351 648 ... 678 438 402]\n",
      "[ 723  792   25 ... 1407  104  189]\n",
      "[ 310  737  474 ... 1096  944  572]\n",
      "[ 237  505 1148 ...  582  106  650]\n",
      "[1276 1071 1323 ...   34  324  107]\n",
      "[432 625 171 ... 108 418 327]\n",
      "[1019 1372 1316 ... 1096  116  672]\n",
      "[1305 1345  286 ...  682  624  110]\n",
      "[1276  541   17 ...  377  668  111]\n",
      "[ 998  891 1071 ...  650  112   86]\n",
      "[ 460  679 1228 ...    4  361  618]\n",
      "[1347  962 1282 ...  508  888  114]\n",
      "[1211  169 1261 ...   37 1164  115]\n",
      "[ 260  931 1126 ... 1358  608  666]\n",
      "[  84 1074 1080 ...  887  410  117]\n",
      "[ 750  390 1434 ...  220  448  118]\n",
      "[ 653  559 1110 ...  873  119  504]\n",
      "[1032 1347  936 ... 1376 1319  309]\n",
      "[ 429  711  571 ... 1083 1319  121]\n",
      "[178 954  57 ... 468 556 839]\n",
      "[ 222   84 1072 ...  822 1341  123]\n",
      "[  16  339  651 ...  572  944 1090]\n",
      "[ 180  695  982 ... 1096  572 1090]\n",
      "[538 564 187 ... 256 236 846]\n",
      "[854 392 498 ... 150 762 127]\n",
      "[ 166  890  818 ...  502 1097 1333]\n",
      "[1126  685   84 ...  434  140   14]\n",
      "[ 285 1316  339 ...  572  116 1090]\n",
      "[ 218 1073  636 ...  641 1398  131]\n",
      "[ 392 1282  193 ...  107   13   34]\n",
      "[1152  716 1250 ...  707  538 1246]\n",
      "[  12  827  906 ...  346 1091  427]\n",
      "[1005  313  797 ...  135 1341 1270]\n",
      "[265 413 228 ... 950 106 136]\n",
      "[ 595 1026  156 ... 1341 1163  137]\n",
      "[ 328  603  856 ... 1050   19  138]\n",
      "[1103  222  615 ... 1164  706  139]\n",
      "[1076 1092 1126 ...  715   14  140]\n",
      "[1373   52 1407 ...  920 1124  141]\n",
      "[ 941  328 1180 ...  285  872  142]\n",
      "[456 476 968 ... 296 842 143]\n",
      "[  90  813 1376 ...  507  144  988]\n",
      "[  33 1304  158 ...  303 1271  116]\n",
      "[1071  998 1343 ... 1342  146  860]\n",
      "[ 315 1406 1067 ...  990  857  147]\n",
      "[ 270  833 1361 ...  664  836  148]\n",
      "[  84  350 1023 ... 1220  854  149]\n",
      "[ 743  678 1049 ...  762  127  485]\n",
      "[  16   18  432 ...  418 1165  151]\n",
      "[ 925 1276  581 ...  674 1342  334]\n",
      "[1100  192  363 ...  410  578  562]\n",
      "[   7 1326  218 ...  754  116 1090]\n",
      "[679  16 611 ... 629 155 591]\n",
      "[ 176 1276  122 ...  156  982  874]\n",
      "[  64 1139  418 ...   89  582   86]\n",
      "[ 953  792  432 ... 1124 1342 1188]\n",
      "[994  84 462 ... 159 948 252]\n",
      "[  84 1021  491 ...  123  160   43]\n",
      "[ 255 1318   19 ... 1111  299  161]\n",
      "[1227  707  591 ...  675 1307  162]\n",
      "[ 525  823  451 ... 1088  163  213]\n",
      "[1417  970 1415 ... 1357  507  846]\n",
      "[ 339 1082 1071 ... 1333 1097  165]\n",
      "[1087  814  777 ...  166 1274  496]\n",
      "[1259  671  839 ...  198  447  167]\n",
      "[ 194  167  679 ... 1151  944  102]\n",
      "[ 414 1129 1082 ...  955  274  169]\n",
      "[ 998 1352 1071 ...  582  439  170]\n",
      "[ 136  376  440 ... 1320 1062 1089]\n",
      "[ 732   70  262 ... 1246  283  172]\n",
      "[ 964  636 1228 ...  185  622  173]\n",
      "[308 879  81 ... 680 904 174]\n",
      "[376 491 862 ... 708 475 175]\n",
      "[1115  170  338 ...  206 1213  176]\n",
      "[ 640  262   70 ...  368 1200  691]\n",
      "[917 140   8 ... 262 460 220]\n",
      "[1008  490  971 ...  512  896 1342]\n",
      "[891 574 376 ... 651 389 180]\n",
      "[ 441  603   53 ... 1425  522  181]\n",
      "[ 717  283  453 ...  642 1312  182]\n",
      "[1323  611 1080 ...  674  252   94]\n",
      "[ 432 1323  491 ... 1064  184 1122]\n",
      "[474 612  16 ... 836 173 185]\n",
      "[145 714 352 ... 409 582 186]\n",
      "[1239  432  162 ...  456  698  187]\n",
      "[1294  723  145 ...   60 1060  377]\n",
      "[1323   18  856 ... 1275 1016  189]\n",
      "[ 432 1129  836 ...  870  746  190]\n",
      "[1080   84 1323 ...  111  200  191]\n",
      "[308 555 890 ... 101 220 192]\n",
      "[1114  555  856 ...  255  999  193]\n",
      "[1110  559  670 ...  790  977  194]\n",
      "[ 734  949 1171 ...  195  683 1266]\n",
      "[ 739  307  773 ...  658 1125  196]\n",
      "[1300  784  458 ...  197  772  777]\n",
      "[ 920  360 1083 ...  253  822  198]\n",
      "[ 234  126  241 ...  481 1402  199]\n",
      "[ 222  432   52 ...  191 1428  200]\n",
      "[970 545 961 ... 102 368 967]\n",
      "[1235  178  487 ... 1060   23  202]\n",
      "[ 596  941 1155 ...  107 1052 1387]\n",
      "[ 421  564 1053 ...  162  675  204]\n",
      "[ 803  218 1371 ...  365  593  205]\n",
      "[ 141  979  218 ...  719 1163  206]\n",
      "[  47 1344  923 ... 1383  824  934]\n",
      "[ 935  866 1258 ...  147  857 1144]\n",
      "[ 777 1011 1367 ... 1371  811 1414]\n",
      "[ 339 1337  302 ...  492 1072 1089]\n",
      "[ 308 1218  369 ... 1047  234  853]\n",
      "[751 339 682 ... 540 728 212]\n",
      "[525 602 945 ... 609 220 213]\n",
      "[  70 1069  337 ...  641 1361  214]\n",
      "[ 120  421 1019 ...  950  189  104]\n",
      "[ 800 1170  112 ... 1090  361  116]\n",
      "[525 929 788 ... 678 213 217]\n",
      "[ 401 1008  627 ... 1138  772  218]\n",
      "[1073  926  824 ...   43  820  855]\n",
      "[917 300 528 ... 956 412 220]\n",
      "[970 141 260 ... 273 221 244]\n",
      "[1071  525  735 ...   86 1253 1229]\n",
      "[ 783  735 1071 ...  443  223  964]\n",
      "[616 735  65 ... 263 850 224]\n",
      "[ 467 1389 1319 ... 1050  629  576]\n",
      "[ 750  454  339 ... 1333   91  226]\n",
      "[180 943 759 ... 532 944 227]\n",
      "[ 498  512  956 ... 1391 1411  228]\n",
      "[ 941 1293 1093 ...  582  631  229]\n",
      "[218 180 712 ... 967 672 586]\n",
      "[ 262 1361  330 ... 1012  370  231]\n",
      "[169  64 544 ... 678 443 232]\n",
      "[1409  525  429 ... 1299  189  233]\n",
      "[1426  135  809 ... 1144 1174  234]\n",
      "[1071  941 1102 ...   52  263  235]\n",
      "[ 556 1107 1370 ...  236 1271 1357]\n",
      "[ 552  460  940 ... 1027 1172 1054]\n",
      "[ 447 1241  615 ...  168  368  102]\n",
      "[525 890 906 ... 868 249 239]\n",
      "[ 362  968  295 ...  886 1182  240]\n",
      "[ 117  308 1431 ... 1047  241  246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1006  534 1371 ...  932  738  242]\n",
      "[1032   84 1085 ...  794  283  243]\n",
      "[ 222  270  260 ...  988 1358  244]\n",
      "[ 926 1383  241 ...  630 1296  245]\n",
      "[ 308  307 1324 ...   99  381  246]\n",
      "[826 465 682 ... 689 474 247]\n",
      "[ 862   76  285 ...  808 1062  515]\n",
      "[998 851 162 ...  19  86 249]\n",
      "[1228  949  865 ... 1178 1266  683]\n",
      "[1073    7  949 ... 1362 1296  251]\n",
      "[ 611  262  121 ... 1259   94  252]\n",
      "[ 780  460  446 ...  822 1198  253]\n",
      "[1071  748   70 ... 1414    2  345]\n",
      "[ 429  711  204 ... 1134 1053  255]\n",
      "[ 516  194 1157 ...  102 1090  572]\n",
      "[ 989  519  145 ... 1342 1436  257]\n",
      "[ 678  934  597 ... 1316 1246  258]\n",
      "[1044  729  411 ...  705  259  608]\n",
      "[ 663 1026  142 ...  981 1367  346]\n",
      "[1073 1261 1430 ... 1018  641 1328]\n",
      "[ 505 1079  970 ...  982  486  262]\n",
      "[1265 1012  790 ...  358  850  263]\n",
      "[597 257 876 ... 470 444 264]\n",
      "[1350  994  732 ...  256 1434  265]\n",
      "[ 544  222  873 ...  935  175 1258]\n",
      "[ 181  803  274 ... 1062 1089  267]\n",
      "[ 944 1193  958 ... 1048  798  268]\n",
      "[ 233 1275   89 ...  572 1096  116]\n",
      "[1020  590  157 ... 1134  871  270]\n",
      "[ 156  663  485 ...  641  271 1137]\n",
      "[1055 1226 1274 ... 1316  772 1415]\n",
      "[1046  506   93 ...   39 1176  273]\n",
      "[  65  110  978 ...   89 1302  274]\n",
      "[ 320   72 1093 ...  999  510  275]\n",
      "[1071  941  419 ...  589  585  276]\n",
      "[1276  523   84 ...  631  294  481]\n",
      "[ 273 1348  931 ...  661 1270 1075]\n",
      "[ 998  308 1071 ...  933  561  279]\n",
      "[1080  485 1093 ...  271 1403  280]\n",
      "[ 799  431  141 ...  857  147 1057]\n",
      "[194 994 156 ... 708 332 282]\n",
      "[ 869  412 1430 ...  391  917  283]\n",
      "[ 482 1289  512 ...  333  586  154]\n",
      "[ 914  856 1008 ... 1123  297  285]\n",
      "[545 798 821 ... 960 958 286]\n",
      "[ 413 1193 1208 ...  294  481  287]\n",
      "[1071  587  603 ... 1098 1188  470]\n",
      "[1311  723 1008 ...  314 1235  930]\n",
      "[ 318 1347  639 ...  896 1204  290]\n",
      "[   7  803 1228 ...  622  341  291]\n",
      "[ 224  262  412 ... 1110  710  292]\n",
      "[917 410 825 ... 162 448 293]\n",
      "[ 953  143 1175 ...  804 1188  294]\n",
      "[1002  871  982 ...  600  111 1060]\n",
      "[476 456 798 ... 443  97 296]\n",
      "[ 475 1007 1323 ...  438  297 1342]\n",
      "[772 290 429 ...   2 795 298]\n",
      "[ 615 1310  662 ...  830  169  299]\n",
      "[ 619  711 1382 ...  824  790  300]\n",
      "[   7  653   89 ...  572  116 1090]\n",
      "[1100  693 1330 ...   14  887  302]\n",
      "[ 112  186 1157 ...  672  333  488]\n",
      "[ 478  486  318 ...  446 1138  304]\n",
      "[1074   46  932 ... 1253  305  106]\n",
      "[ 222 1032  710 ...  517  375  306]\n",
      "[ 635 1268  646 ...   34  307  107]\n",
      "[1433  892  740 ...  840  920  308]\n",
      "[  49 1227 1062 ... 1175 1376  309]\n",
      "[1032  891   81 ...  310  717  763]\n",
      "[ 325  178  848 ... 1414   50  311]\n",
      "[ 340 1274 1365 ...  669   23  661]\n",
      "[ 660 1161 1300 ...  622 1352 1042]\n",
      "[ 597  685  363 ... 1009  314   36]\n",
      "[ 456  720  589 ... 1178 1266  683]\n",
      "[ 611 1436  653 ...  116  368 1090]\n",
      "[ 457  364  907 ... 1133 1297 1283]\n",
      "[679 304  29 ... 162 989 318]\n",
      "[ 548 1191  476 ...  319  849  944]\n",
      "[ 659  998  941 ...  927  804 1161]\n",
      "[ 932 1327    8 ...   39  274  321]\n",
      "[ 145   66  503 ... 1094 1157  322]\n",
      "[ 775 1069  509 ...  891  913 1416]\n",
      "[ 786  560  339 ... 1013 1238  324]\n",
      "[1148 1067  597 ... 1174   52  447]\n",
      "[  16 1164 1080 ...  116  944 1090]\n",
      "[1305   98  305 ...  108   64  327]\n",
      "[1225  110  722 ...  265 1320  328]\n",
      "[ 356 1226 1267 ...  405 1436  329]\n",
      "[ 482  545 1431 ...  216    4  944]\n",
      "[ 404 1276 1182 ... 1255  483  331]\n",
      "[1267  799 1080 ...  674 1168  332]\n",
      "[ 194   16  180 ...  683 1090  116]\n",
      "[ 432  308  178 ... 1204 1342  334]\n",
      "[ 167 1164  679 ... 1060  669  335]\n",
      "[ 270 1347  987 ... 1131 1012  336]\n",
      "[ 555 1274  548 ...  346  246 1104]\n",
      "[   8  166 1274 ...  338  950  964]\n",
      "[1300 1280  209 ...   24  742  339]\n",
      "[ 635 1173  143 ...  107  963  340]\n",
      "[1323 1326  964 ...  173  622  341]\n",
      "[ 398  241 1340 ... 1231 1266  683]\n",
      "[1228  581  460 ...  944  623  575]\n",
      "[1100  667 1330 ...  703  242  932]\n",
      "[1121  989 1005 ...   13    2  345]\n",
      "[ 790  968 1274 ... 1058  346 1091]\n",
      "[624 696 376 ... 815 540 347]\n",
      "[ 663   84 1408 ...  419  348 1087]\n",
      "[1279  180 1061 ...  480  349  574]\n",
      "[1343  486 1155 ...  811  917  350]\n",
      "[432 690 563 ... 813 811 351]\n",
      "[1341 1107 1159 ...  944  333  572]\n",
      "[ 651  339  978 ... 1378 1398  353]\n",
      "[1185  211  432 ...  688  354   33]\n",
      "[  70  448  660 ...  579 1196  562]\n",
      "[ 866 1382  319 ...  356 1253 1288]\n",
      "[ 474  320  382 ... 1083  616  357]\n",
      "[1217  495 1225 ...  662  342  358]\n",
      "[1161  141 1313 ...  479  359  683]\n",
      "[1323 1241   84 ... 1215  150  360]\n",
      "[ 167  308 1399 ...  361    4  944]\n",
      "[ 84 743 144 ... 207 883 362]\n",
      "[ 562 1196  418 ...  589  693  406]\n",
      "[1361  970  455 ... 1356  662  364]\n",
      "[803 545 961 ... 859 131 365]\n",
      "[ 755  799  991 ... 1174 1189  366]\n",
      "[ 413 1310  611 ...  631  568  367]\n",
      "[ 611  180 1164 ...  808  532  102]\n",
      "[1419  452  719 ...  404 1365  369]\n",
      "[  69 1268 1347 ... 1056  794  370]\n",
      "[ 679  759  337 ... 1068   47  371]\n",
      "[1155   79  961 ...  948  294  372]\n",
      "[  60  141 1094 ...  502  722  373]\n",
      "[ 611  982 1300 ...   94 1162  291]\n",
      "[ 307  554  929 ...  449 1058 1157]\n",
      "[  49  831  422 ... 1369 1057 1376]\n",
      "[ 237 1294 1409 ...   72  668  377]\n",
      "[ 413  308  194 ... 1092 1410  378]\n",
      "[ 653  871  233 ... 1321  124  102]\n",
      "[ 762  716 1157 ...  829  788  380]\n",
      "[671 654 735 ... 850 235 381]\n",
      "[ 998  783 1008 ...  433 1342 1103]\n",
      "[ 581 1267  879 ...  446  383  678]\n",
      "[1073  550  423 ... 1361  920  384]\n",
      "[1294 1071 1018 ...  324 1238  385]\n",
      "[1323 1241 1228 ... 1085 1357  386]\n",
      "[ 195  145  352 ... 1236  644  387]\n",
      "[ 433   33  188 ...  846 1357  388]\n",
      "[843 491 462 ... 661 669 389]\n",
      "[ 545 1141 1164 ...  124  102  368]\n",
      "[  84 1293  128 ... 1056  391  283]\n",
      "[1282  937  755 ...   56  486  392]\n",
      "[560 660 128 ... 648 730 393]\n",
      "[ 732 1372   60 ...  731  681  394]\n",
      "[369 545 235 ... 572 401 849]\n",
      "[576 493 283 ... 232 443 396]\n",
      "[ 526 1030  399 ...  411  434  397]\n",
      "[1241  432  457 ... 1075 1127  398]\n",
      "[ 856  491 1310 ...  409  916  399]\n",
      "[ 195  783  465 ... 1342  720  400]\n",
      "[557 218 772 ... 975 899 672]\n",
      "[ 946  351 1023 ...  933  678  402]\n",
      "[ 891  466 1403 ... 1206  772  403]\n",
      "[1419  985  429 ...  855  404  369]\n",
      "[ 829 1267  788 ... 1436 1170  405]\n",
      "[1071  623  424 ... 1229   86  406]\n",
      "[528 711 237 ... 561 407 446]\n",
      "[369 308 432 ... 294  22 408]\n",
      "[ 457  151  364 ...  433 1342  409]\n",
      "[1126 1282  128 ...  896 1410  410]\n",
      "[1135  392 1203 ...  397 1270  411]\n",
      "[ 185  194  799 ... 1173  220  412]\n",
      "[ 996 1146  537 ...  678 1204  886]\n",
      "[ 819 1132  408 ...  553  872  414]\n",
      "[  96  998 1415 ...  553  961  415]\n",
      "[1405 1179  211 ...  906  481  416]\n",
      "[1420  376  557 ... 1146 1414  417]\n",
      "[1228 1323  491 ...  151  418 1165]\n",
      "[  39  589  585 ... 1087  419  851]\n",
      "[ 404   18  318 ... 1374  225  420]\n",
      "[1126  597  376 ... 1242    2  421]\n",
      "[1375  994  178 ...  422  398 1127]\n",
      "[1071  998  792 ...  502   26  950]\n",
      "[300 824 197 ... 459 124 368]\n",
      "[ 890  183  489 ...   19 1199  425]\n",
      "[ 325  164  262 ... 1056  794  426]\n",
      "[ 671  998 1071 ...  921 1199   86]\n",
      "[ 467  161 1148 ... 1138   21  428]\n",
      "[ 902 1077 1129 ... 1120 1239  429]\n",
      "[1289 1235  329 ...  532    4  116]\n",
      "[   1 1239  363 ...  896  630  431]\n",
      "[ 190  743  742 ...  432 1284 1336]\n",
      "[  84  788  115 ... 1204  433 1342]\n",
      "[1345  290 1072 ...  715  140  434]\n",
      "[  54  998 1148 ...  896  991  435]\n",
      "[ 141 1080  343 ...  787  703  436]\n",
      "[ 621  874  156 ... 1121  616 1413]\n",
      "[ 318  115 1007 ... 1204 1342  438]\n",
      "[ 525  315 1352 ...  721  170  439]\n",
      "[ 611  421  166 ... 1162   94  440]\n",
      "[798 486 564 ... 678 443 441]\n",
      "[156 603  48 ... 426 309 442]\n",
      "[1023 1037  525 ...  441  678  443]\n",
      "[ 864  818 1250 ...  502   26  444]\n",
      "[ 104 1159  117 ...  641   78  445]\n",
      "[905 731 877 ... 938 873 446]\n",
      "[671 237 936 ... 653  52 447]\n",
      "[ 622  735  917 ... 1024  867  448]\n",
      "[ 414 1352   84 ...  175  580  449]\n",
      "[1005  371  550 ...  650  426   28]\n",
      "[1069   31 1141 ... 1090  124  691]\n",
      "[1276 1431   84 ...  624 1367  410]\n",
      "[  33  940  628 ...  359 1357  846]\n",
      "[1228  457  584 ...  510  275  895]\n",
      "[ 184 1111 1129 ...  538 1029  255]\n",
      "[633 204 590 ... 889  86 456]\n",
      "[1067  350  152 ... 1155 1369  457]\n",
      "[1282   25  257 ...  547   54  458]\n",
      "[1417  970  695 ...  236  846  524]\n",
      "[ 735 1071  646 ...   39  446  460]\n",
      "[1323   69 1375 ...  752 1246 1160]\n",
      "[ 865  900   31 ... 1195  362  462]\n",
      "[ 535 1422  851 ... 1238  456  536]\n",
      "[ 706 1129  792 ... 1104   45   54]\n",
      "[194 268 613 ... 333 754 116]\n",
      "[ 307  710 1421 ...  448  612  466]\n",
      "[611 695 404 ... 755 799 467]\n",
      "[1232  432  615 ... 1298  445  468]\n",
      "[ 431 1365  404 ...    4 1090  572]\n",
      "[ 706  719 1359 ...  920  297  470]\n",
      "[1261 1430 1096 ...  499  839  471]\n",
      "[  12  538 1017 ...  116  125 1090]\n",
      "[917 792 110 ... 293 693 473]\n",
      "[ 569 1155    8 ... 1098  865  474]\n",
      "[611 907 220 ... 463  23 475]\n",
      "[ 226  128  441 ... 1227  106  476]\n",
      "[ 322   50  949 ... 1111  436  477]\n",
      "[ 938 1100  660 ... 1413  302 1227]\n",
      "[ 521 1167  857 ... 1178 1266  683]\n",
      "[1373 1355  218 ...  480  349  574]\n",
      "[   9 1347   84 ...  294  948  481]\n",
      "[ 420 1384  711 ...  512 1302  482]\n",
      "[1080  554  692 ...  409 1342  483]\n",
      "[ 855  584 1354 ...  426  896 1158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  26  502 1350 ...  150  762  485]\n",
      "[1346 1361  892 ...  698  262  486]\n",
      "[ 554  491  237 ... 1158  800  487]\n",
      "[ 180  912 1341 ... 1096  116 1090]\n",
      "[ 99 970 972 ... 601 273 489]\n",
      "[ 339  679 1106 ... 1416 1027 1054]\n",
      "[ 185  173   65 ... 1302  305  600]\n",
      "[1080  525  268 ...  220  878 1261]\n",
      "[ 847 1340  320 ...  493 1329 1362]\n",
      "[ 312  914  237 ... 1124  556 1249]\n",
      "[1323  491   69 ...  495 1410  948]\n",
      "[640  24 842 ...   2 166 496]\n",
      "[1282  750  666 ...  381 1177  497]\n",
      "[ 733  727  228 ...  948 1115  481]\n",
      "[ 555  698  181 ...   97  383 1091]\n",
      "[1323  939  856 ... 1246 1237  500]\n",
      "[ 228  818 1244 ...   95  501  896]\n",
      "[ 241 1343  228 ... 1016   26  502]\n",
      "[ 187  222  533 ...  846  503 1357]\n",
      "[ 506 1382  555 ...   20  119  504]\n",
      "[695 167 557 ... 216 361 944]\n",
      "[1277  552 1026 ...  760  974  506]\n",
      "[630 456 578 ... 359 236 507]\n",
      "[1301  490  549 ...  508  114 1342]\n",
      "[ 851  129  602 ...  246 1104  509]\n",
      "[ 514 1076   72 ...  999  275  510]\n",
      "[1171 1386 1187 ... 1338  292  511]\n",
      "[326 361 513 ... 426 896 512]\n",
      "[  33  867   24 ... 1266  953  359]\n",
      "[ 710 1080  615 ...  299 1428  200]\n",
      "[ 376   49  918 ...  336 1089  515]\n",
      "[ 792 1095  237 ...   24  896  516]\n",
      "[1232   56  491 ... 1120  375  517]\n",
      "[1323  194  615 ...  146  518  378]\n",
      "[ 474  112 1073 ... 1361   78  519]\n",
      "[1426  921  387 ... 1198 1224  520]\n",
      "[ 989  457  710 ... 1204  409  521]\n",
      "[ 640  978 1211 ...  656  181  522]\n",
      "[ 395 1428 1354 ...  425 1407  523]\n",
      "[ 218    7  803 ...  846  236 1357]\n",
      "[ 59 625 363 ... 219 185 299]\n",
      "[ 491  314  653 ...  210 1124  526]\n",
      "[ 851  847   45 ...  181 1426 1221]\n",
      "[ 615  732  262 ...  283 1393  495]\n",
      "[ 639  176 1071 ... 1256 1304  529]\n",
      "[ 376 1340  312 ...  963 1348 1336]\n",
      "[827 537 418 ... 950 964 531]\n",
      "[ 308 1399  843 ...  618  532    4]\n",
      "[ 799 1353  891 ...   61 1261  533]\n",
      "[ 615  288  444 ... 1158  332  534]\n",
      "[1323 1126  277 ... 1266 1357  535]\n",
      "[1351  851  679 ...   71  991  536]\n",
      "[ 178  612   70 ...  697  537 1099]\n",
      "[ 467 1000  339 ...  794 1056  538]\n",
      "[ 711 1185  914 ...  760  539 1048]\n",
      "[1362 1329  459 ...  570  897  540]\n",
      "[ 475  891  723 ... 1061  426  896]\n",
      "[ 580   40 1163 ...  592  306 1411]\n",
      "[ 748  603 1184 ... 1188  294  543]\n",
      "[1129  478 1126 ... 1206  675  544]\n",
      "[1337  194  544 ...   70  850  545]\n",
      "[ 608  914  799 ... 1261  717  893]\n",
      "[ 461  978 1435 ...  573  249  547]\n",
      "[1277 1139 1323 ...  370   10  548]\n",
      "[679 545 188 ... 116 532 216]\n",
      "[729 555 345 ... 777  70 550]\n",
      "[545 720 433 ... 177 102 261]\n",
      "[ 597 1102  706 ... 1436 1170  552]\n",
      "[978 832 308 ... 502  26 553]\n",
      "[1228 1370  392 ...   50  554  185]\n",
      "[ 322   97  560 ... 1375  929  665]\n",
      "[ 478  855 1155 ... 1249  556  294]\n",
      "[  77 1340 1419 ... 1113  557 1009]\n",
      "[1241   81  447 ...  481  558  948]\n",
      "[1352  951  390 ...  991 1110  559]\n",
      "[1362 1329   50 ...  213  531  560]\n",
      "[998 663 866 ... 756 561 933]\n",
      "[1241 1228 1323 ... 1410 1196  562]\n",
      "[ 471 1412  870 ... 1089  563  267]\n",
      "[1257  766 1041 ...  791   14  140]\n",
      "[ 803  578  539 ...  197  705 1358]\n",
      "[838 281 340 ... 718 573 566]\n",
      "[1041  719   72 ...  717  896  289]\n",
      "[1093  962 1032 ...  372   28  568]\n",
      "[ 865  337  777 ...  569 1258  935]\n",
      "[ 764  205  788 ...  540 1380  570]\n",
      "[ 308 1272  949 ...  674   94  334]\n",
      "[989 998 671 ... 974 232 747]\n",
      "[ 829  929  693 ... 1097   26  502]\n",
      "[ 31 774   5 ... 480 349 574]\n",
      "[ 546  915  854 ... 1273 1380  872]\n",
      "[1074  128   84 ... 1050   24  576]\n",
      "[  65  198  748 ...  644   83 1188]\n",
      "[1071  998  706 ... 1253  199  578]\n",
      "[ 941 1185  318 ... 1342 1436 1103]\n",
      "[1323   84  884 ...  760 1242    2]\n",
      "[ 755  600 1363 ...  473 1392  581]\n",
      "[   9  318 1241 ... 1256  229  582]\n",
      "[ 798  613  554 ... 1178 1266  683]\n",
      "[ 141  120 1365 ...  134   91  584]\n",
      "[ 262 1352  404 ... 1161  170  276]\n",
      "[ 874  188    7 ... 1090  116  586]\n",
      "[ 939  501 1395 ...  797  587 1062]\n",
      "[ 491  325  248 ... 1260 1048  588]\n",
      "[1352  989 1071 ...  276  589  585]\n",
      "[ 536  760  539 ...  162 1307  633]\n",
      "[1100  633   16 ...  579 1113  591]\n",
      "[ 392 1217  581 ... 1060  943  668]\n",
      "[ 31 640 611 ... 906 205 593]\n",
      "[ 819 1132 1076 ...  829 1352  414]\n",
      "[ 259 1314  914 ... 1302  121  595]\n",
      "[ 194 1164  818 ...  469 1090  919]\n",
      "[ 786   16  611 ... 1406  926  597]\n",
      "[1065  392 1241 ...  275  510 1204]\n",
      "[ 662  856  491 ... 1204 1278  599]\n",
      "[1276 1310  432 ... 1060  409  600]\n",
      "[659 510 463 ... 502  26 950]\n",
      "[1209 1305 1368 ...  862  602 1211]\n",
      "[254 611 454 ... 624 682 291]\n",
      "[ 867  949  218 ... 1357  846  953]\n",
      "[ 653  369  314 ...  556 1052  605]\n",
      "[ 695  943  101 ... 1271 1096  944]\n",
      "[1107 1037 1023 ...  607   38 1320]\n",
      "[1431  923  733 ... 1166  259  608]\n",
      "[ 735 1353  172 ... 1295  213  609]\n",
      "[ 345   34   29 ...  550 1367  610]\n",
      "[1268  237 1126 ...  650  516  611]\n",
      "[ 622 1071  922 ... 1342 1394   89]\n",
      "[730 555 735 ... 938 263 613]\n",
      "[584 854 873 ... 249 641 614]\n",
      "[622 735 463 ... 690 904 448]\n",
      "[1086  733    7 ...  978  357  616]\n",
      "[1428  569  173 ...  213 1302  650]\n",
      "[1238  822  273 ...  618  361  944]\n",
      "[1100   17  848 ... 1410  629 1196]\n",
      "[1012  538  841 ...  684  359  683]\n",
      "[ 525  616  890 ...  964 1349  621]\n",
      "[ 491  531 1279 ...  929 1042  622]\n",
      "[1323  615 1431 ... 1090  849 1096]\n",
      "[ 223  617 1080 ...  146  599  624]\n",
      "[ 735  183 1071 ...  438  531  964]\n",
      "[1326  339 1094 ... 1357  401  386]\n",
      "[112 233 186 ... 256 586 125]\n",
      "[1095 1106  781 ... 1017 1302  628]\n",
      "[ 413  949   84 ...  410 1410  629]\n",
      "[1126 1074  711 ... 1056  794  630]\n",
      "[ 855 1178   84 ... 1363 1256  631]\n",
      "[ 826  998 1071 ...  632  529 1161]\n",
      "[1222 1013   71 ...  100  633 1307]\n",
      "[ 653 1094  447 ... 1254  532  944]\n",
      "[ 890  839  340 ...  861 1057 1369]\n",
      "[ 830  316  418 ... 1034  833  636]\n",
      "[ 414  799 1116 ...  146 1304  637]\n",
      "[545  33 533 ... 623 116 638]\n",
      "[819 431 340 ... 944 639  38]\n",
      "[320  84 603 ... 611  97 296]\n",
      "[1026  883 1129 ...  249  271  641]\n",
      "[ 70 945 736 ... 345 107 642]\n",
      "[ 474  156 1241 ...  398  643  853]\n",
      "[  70 1323  325 ...  912  644  920]\n",
      "[ 920  431  369 ... 1436  842  645]\n",
      "[ 951  476  767 ...  309  696 1393]\n",
      "[ 515  302  932 ... 1094 1249  305]\n",
      "[ 603 1154 1340 ... 1284  393  648]\n",
      "[1280  178 1014 ... 1099  334  948]\n",
      "[ 318  451  783 ... 1188  526  650]\n",
      "[ 359  503  855 ... 1358  180  651]\n",
      "[ 720  734  400 ...  846  652 1357]\n",
      "[1409  228  307 ...  426 1158  442]\n",
      "[ 218  423  425 ...  642 1136  654]\n",
      "[ 432 1323  855 ...  674 1168  655]\n",
      "[563  84 392 ... 836  89 656]\n",
      "[ 564 1054  128 ...   66 1357  846]\n",
      "[ 422  985  335 ... 1361 1125  658]\n",
      "[311 476 506 ... 116 256 815]\n",
      "[ 528  929  998 ...   89 1394  660]\n",
      "[994 523 663 ... 389 675 661]\n",
      "[ 350 1079 1007 ...   56  580  662]\n",
      "[445  85 429 ... 707 289 663]\n",
      "[1232 1013  653 ...  299  148  664]\n",
      "[ 132 1020  241 ...  906 1218  665]\n",
      "[564  89   7 ... 116 754 586]\n",
      "[ 998 1071  616 ... 1034  449  667]\n",
      "[ 879  540  530 ...  239  249 1378]\n",
      "[ 273  581 1176 ...  943  668  669]\n",
      "[1185  799  914 ...   24  670  950]\n",
      "[ 178  486  567 ... 1162   78  671]\n",
      "[ 194   72  613 ... 1096  488  401]\n",
      "[376 415 670 ... 799 673 755]\n",
      "[1080 1310  974 ...   94  332  674]\n",
      "[ 803 1241   84 ...  377    1  675]\n",
      "[ 733 1343  262 ...  481  676  495]\n",
      "[ 178  907 1310 ...  836  677 1099]\n",
      "[ 974  562 1051 ... 1112  428  322]\n",
      "[1422  645  815 ...  511  974  578]\n",
      "[ 176 1032  432 ... 1387  332  680]\n",
      "[1326  317  295 ...  386 1062  681]\n",
      "[1238  523   67 ...  624  110  682]\n",
      "[ 508  888  628 ... 1178 1266  683]\n",
      "[1326  912 1370 ...  488  919 1090]\n",
      "[ 211 1005 1112 ... 1324  685 1223]\n",
      "[ 712   86  950 ... 1004  622  686]\n",
      "[429  84 942 ... 717 663 289]\n",
      "[ 525 1323 1071 ...  481  146  688]\n",
      "[ 432 1352  525 ...  508  114  689]\n",
      "[ 825  378  350 ...  956 1158  867]\n",
      "[167 428 628 ... 368 261 102]\n",
      "[968 188 187 ... 236 116 572]\n",
      "[ 566  178 1236 ... 1218  473  693]\n",
      "[ 530  471  761 ... 1389  883  694]\n",
      "[ 606  459 1007 ... 1372   60  695]\n",
      "[562 799 968 ... 756 309 696]\n",
      "[1080  994  428 ...  674  912  697]\n",
      "[1361 1193  779 ...  993   33  650]\n",
      "[ 783 1268 1301 ...  650  699 1342]\n",
      "[ 421 1080 1032 ...  712  947  700]\n",
      "[ 614 1427  421 ...  295 1312  701]\n",
      "[640 167 613 ... 618 928 368]\n",
      "[ 329  912 1121 ...  787  813  703]\n",
      "[  69  552 1145 ...  944 1090  704]\n",
      "[ 803  923  578 ...  608 1358  705]\n",
      "[ 199  832 1423 ... 1178  139  706]\n",
      "[ 178  531 1340 ...  251  707  840]\n",
      "[ 245  541  432 ... 1428 1279  708]\n",
      "[ 357   65 1012 ...  927 1426 1350]\n",
      "[1020   65  748 ...  991  292  710]\n",
      "[843 611 435 ... 674  94 629]\n",
      "[ 237 1074  692 ... 1094  186  712]\n",
      "[1175  180  339 ... 1317  684 1312]\n",
      "[ 637   10  656 ...  497  714 1334]\n",
      "[291 374 671 ... 434 990 715]\n",
      "[1221 1274  166 ...  267  346 1089]\n",
      "[ 128   72 1347 ... 1415  896  717]\n",
      "[ 141  257  746 ... 1118   54  718]\n",
      "[ 141 1417  257 ...  246  719 1163]\n",
      "[ 173   65  318 ...  896 1342  720]\n",
      "[ 525  986 1409 ... 1298  439  721]\n",
      "[ 103   70  560 ...  172  813 1246]\n",
      "[ 357 1427  648 ...  295  249  365]\n",
      "[ 799  790 1324 ...  724 1097  165]\n",
      "[640  31 635 ... 181 906 522]\n",
      "[1139 1017  260 ...   67  925  726]\n",
      "[1235  222 1011 ...  828  727 1146]\n",
      "[ 158 1061 1115 ...  897  540  728]\n",
      "[1032  866 1323 ...  729  483 1342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[560 531 322 ... 951 393 730]\n",
      "[222  69 890 ... 877 814 731]\n",
      "[799 537 602 ... 732 950 964]\n",
      "[ 418 1327  752 ...   24  377  780]\n",
      "[ 302 1353  917 ... 1173  734  220]\n",
      "[ 660  224  615 ... 1227   46 1048]\n",
      "[1178  236 1085 ...    8  736  791]\n",
      "[ 404  308  525 ... 1436  305  737]\n",
      "[770 157 724 ... 738 710 242]\n",
      "[ 695  653 1157 ...  572 1254  944]\n",
      "[ 144  262 1427 ...  573  859  740]\n",
      "[ 148  568  836 ... 1357  453 1416]\n",
      "[1092  536 1375 ...  751  756  742]\n",
      "[ 920 1084  254 ...  777  337  743]\n",
      "[ 178 1050  171 ...  327  616 1031]\n",
      "[ 224 1100   84 ...  791 1155  745]\n",
      "[ 102 1151  177 ... 1289  409  746]\n",
      "[ 671 1239 1340 ...  974  871  747]\n",
      "[ 578  461 1417 ...  882 1016  748]\n",
      "[ 491  392  178 ... 1198  791  749]\n",
      "[ 780 1245  954 ...  978  327  750]\n",
      "[544 685 615 ... 742 756 751]\n",
      "[ 565  337  666 ...  813 1160  752]\n",
      "[556 716 394 ... 846 683 359]\n",
      "[ 180  628  653 ...  333  116 1090]\n",
      "[1261 1065  443 ...  173  799  755]\n",
      "[530 523 432 ... 751 742 756]\n",
      "[ 170  334  629 ...  683 1317  953]\n",
      "[ 998 1203  414 ... 1009 1132  758]\n",
      "[1226  237 1291 ...  668  377  759]\n",
      "[ 525  156 1026 ... 1016  539  760]\n",
      "[ 978 1315 1211 ...  107  802  761]\n",
      "[ 69  70 732 ... 150 485 762]\n",
      "[1403 1280  891 ...  682  978  763]\n",
      "[ 907  653  772 ...  116 1090  919]\n",
      "[ 974  798  194 ...  822 1367  765]\n",
      "[   7  188  233 ...  919  116 1090]\n",
      "[ 216  532 1148 ...   86 1229  767]\n",
      "[ 187 1169   46 ... 1266 1231  826]\n",
      "[ 421  826  413 ... 1302 1342  769]\n",
      "[ 156 1274  133 ...  770  100  668]\n",
      "[ 491 1062 1086 ...  668  771  377]\n",
      "[ 363  221  128 ... 1026  470  772]\n",
      "[149 707 525 ... 668 100 773]\n",
      "[1319  482  883 ... 1062  270  774]\n",
      "[998 360  64 ... 483 232 775]\n",
      "[640 167 404 ... 368 776 967]\n",
      "[ 367  489  525 ... 1091 1367  777]\n",
      "[ 129 1071  998 ...  964  220 1394]\n",
      "[  30   99  777 ...  664 1337  779]\n",
      "[ 120  841  525 ... 1049 1220  780]\n",
      "[ 560 1241 1159 ...  828  622  781]\n",
      "[ 803  710 1129 ... 1341 1203  782]\n",
      "[  52   70 1261 ... 1408  102  967]\n",
      "[ 325 1026 1085 ...  916 1300  784]\n",
      "[1067 1231  325 ... 1336  648  785]\n",
      "[178 615 222 ... 229 175 676]\n",
      "[433 112 494 ... 436 703 787]\n",
      "[1014  190  716 ...  380  829  788]\n",
      "[ 299 1217  885 ... 1361 1161  789]\n",
      "[1027 1354 1226 ...  639  988  790]\n",
      "[1043   78  135 ...  140  715  791]\n",
      "[1235  187  541 ...  256  236  792]\n",
      "[ 530 1142  818 ...  124 1254  754]\n",
      "[1413   84  986 ...  370 1056  794]\n",
      "[1060 1046  141 ... 1266  540  795]\n",
      "[1272 1211  602 ...  631  977 1279]\n",
      "[335 862 742 ... 906 656 797]\n",
      "[ 392  907 1076 ... 1265 1048  798]\n",
      "[1228 1138  590 ...  755  173  799]\n",
      "[ 208  318  892 ... 1170 1014  800]\n",
      "[1355  611  679 ...  569 1258  935]\n",
      "[535 392  62 ... 107  21 802]\n",
      "[ 452  711  236 ... 1300 1238  803]\n",
      "[ 998 1071  748 ...  294  481  804]\n",
      "[ 552 1086  595 ...  539  805 1284]\n",
      "[ 936  429 1323 ...  287 1229  806]\n",
      "[ 322  557  190 ... 1054  807  575]\n",
      "[ 653  262 1094 ...  227  623 1090]\n",
      "[1368  635  262 ... 1238  809 1210]\n",
      "[ 998 1409 1071 ... 1050   86  921]\n",
      "[1323  211  432 ... 1146 1066  811]\n",
      "[ 363   70 1347 ...  674  481  948]\n",
      "[1382 1408  390 ... 1160   90  813]\n",
      "[ 156  308  525 ... 1087 1137 1361]\n",
      "[1069  777 1418 ... 1357  815  388]\n",
      "[1232  222  615 ... 1204  123  816]\n",
      "[ 545  695  940 ...  672  116 1090]\n",
      "[356 830 203 ...  39 190 818]\n",
      "[339 224 616 ... 117 819 797]\n",
      "[1186  994 1350 ...  820 1329 1362]\n",
      "[1257 1435 1253 ...  821  726   67]\n",
      "[ 171  178  784 ...  253 1198  822]\n",
      "[ 308 1053  369 ... 1321 1090  944]\n",
      "[  93 1026  258 ...  207  824  926]\n",
      "[ 178 1109 1187 ... 1421  200  825]\n",
      "[ 491 1276 1115 ... 1358 1413  826]\n",
      "[884  70 144 ... 840 707 827]\n",
      "[1228  833 1232 ... 1281  477  828]\n",
      "[1121  904  443 ... 1324  788  829]\n",
      "[1241 1323  262 ... 1428  299  830]\n",
      "[1323 1340  224 ... 1002  539  760]\n",
      "[ 320  262 1323 ... 1215  127  485]\n",
      "[1126  618 1388 ...  715 1418  833]\n",
      "[ 161  300  494 ...  662  432 1041]\n",
      "[270 460 171 ... 495  43 835]\n",
      "[1244  892  491 ...  664  148  836]\n",
      "[ 308  156 1042 ... 1047 1006 1391]\n",
      "[ 318  866  178 ...  674 1092  838]\n",
      "[1014  233  660 ...  471  499  839]\n",
      "[985 223 939 ... 251 707 840]\n",
      "[ 914  780 1065 ...   14  841 1012]\n",
      "[1067   46  723 ...  904  678 1158]\n",
      "[1130  855  591 ...  107  843  977]\n",
      "[  49  277 1126 ...  791 1198   14]\n",
      "[ 225  882  629 ... 1343  486  845]\n",
      "[ 545  803 1144 ...  437 1357  846]\n",
      "[ 339   56  476 ... 1018  353 1398]\n",
      "[ 23 111   1 ... 791 990 140]\n",
      "[ 476   33  117 ... 1090  572  116]\n",
      "[735 355   2 ... 448 381 850]\n",
      "[ 585  589 1326 ...  419 1087  851]\n",
      "[ 486  489  308 ...  515 1089  852]\n",
      "[458 906 142 ... 624 886 853]\n",
      "[  48  315 1273 ... 1030  492  476]\n",
      "[ 335  304 1073 ...   43  707  855]\n",
      "[ 781   10   46 ...  455  443 1161]\n",
      "[  69 1102  890 ... 1186  147  857]\n",
      "[1323  491  693 ...  917 1160 1246]\n",
      "[1427  705  357 ... 1378  740  859]\n",
      "[ 156  663  369 ... 1013  582  860]\n",
      "[ 920  141  245 ...  263 1369  861]\n",
      "[1282 1032 1347 ... 1188  993 1124]\n",
      "[1157  759  545 ...  960  511  863]\n",
      "[ 576  563 1135 ...  824  883  864]\n",
      "[ 663 1389  478 ... 1170 1098  865]\n",
      "[ 546  915  854 ... 1172  815 1054]\n",
      "[350 799 183 ... 448 220 867]\n",
      "[  56  188 1056 ...  868 1398  353]\n",
      "[  84 1010 1007 ...  990    8  791]\n",
      "[640 427 322 ... 107 416 870]\n",
      "[1239 1337  719 ... 1134  760  539]\n",
      "[ 178   57  597 ... 1404 1411  872]\n",
      "[ 663 1169  803 ...  955 1307  873]\n",
      "[ 815  421 1191 ...  354 1253  874]\n",
      "[291 136 562 ... 332 872  37]\n",
      "[ 264 1413 1023 ...  771 1190  876]\n",
      "[  85 1386  907 ...  252  731  877]\n",
      "[1194  727   78 ...  582  771  878]\n",
      "[  70  994 1359 ...  825  511  920]\n",
      "[ 145  436  752 ...  409 1342  880]\n",
      "[1303  392  432 ... 1137  280  881]\n",
      "[1352   84  262 ...  882 1425  481]\n",
      "[359 583 453 ... 909 999 883]\n",
      "[782 679 161 ... 662 842 143]\n",
      "[ 222  523  491 ... 1393  674  885]\n",
      "[458 581  10 ... 777 853 886]\n",
      "[ 451   79 1080 ...   24  887  688]\n",
      "[1071  466  750 ...  294  508  888]\n",
      "[1000  671  590 ...  538 1338  889]\n",
      "[ 773  224 1430 ...  890 1274  496]\n",
      "[1069  651  427 ...  124  102  891]\n",
      "[1170  938  613 ...  829  593  892]\n",
      "[ 467 1148 1406 ... 1009  557  921]\n",
      "[ 145 1368  237 ...  582  860  894]\n",
      "[698 876 107 ...  24 193  45]\n",
      "[ 318   65 1185 ...  650  426  896]\n",
      "[ 136  234  682 ...  728  347 1416]\n",
      "[1086  114  432 ...  898 1292  718]\n",
      "[  33  257  233 ...  236 1090  116]\n",
      "[1323  363 1241 ...  481  900 1342]\n",
      "[   8  616  646 ...  769 1302  901]\n",
      "[ 855  261   48 ... 1342  902 1077]\n",
      "[ 194 1079 1239 ... 1423  334 1342]\n",
      "[ 934   80  824 ...  609 1295  904]\n",
      "[1171  949  473 ... 1178 1266  683]\n",
      "[ 246  640  777 ...  963  906 1218]\n",
      "[ 463  176 1108 ...  950  964  907]\n",
      "[1282  587  855 ... 1363  674  631]\n",
      "[1279  916  773 ...  883  485  909]\n",
      "[1073  392   33 ...  614  641  910]\n",
      "[ 628  546  915 ... 1027   51 1054]\n",
      "[ 583  753  453 ... 1296  697  912]\n",
      "[1144  434  234 ...  683 1231 1266]\n",
      "[777 796 272 ... 124 702 102]\n",
      "[942 384  37 ... 854 915 546]\n",
      "[ 722 1008  376 ...  912  800  916]\n",
      "[ 81 879 744 ... 700  95 917]\n",
      "[1001  847  208 ...  579   49  918]\n",
      "[ 482  188  968 ...  333  469 1090]\n",
      "[  70  603  318 ...  481  993 1188]\n",
      "[ 855  998  587 ... 1342  644  921]\n",
      "[263 310 628 ... 368 261 702]\n",
      "[ 376  237 1126 ... 1204  923 1436]\n",
      "[1211  525  855 ...  942  872  924]\n",
      "[338 320 223 ... 548 966 925]\n",
      "[ 85 258 331 ... 207 926 824]\n",
      "[587 603 327 ... 522 181 927]\n",
      "[ 653 1053  180 ...  227  623  944]\n",
      "[ 322 1179 1430 ...  906   46  929]\n",
      "[ 367  298 1194 ... 1318  218   24]\n",
      "[1409  788  237 ...  806  931  745]\n",
      "[ 645 1350 1010 ...  434  791  932]\n",
      "[1071  998  866 ...  448  561  933]\n",
      "[  31  938  678 ... 1383 1067  934]\n",
      "[ 865  679  612 ...  569  935 1258]\n",
      "[427  17 407 ... 671  78 936]\n",
      "[ 732  904  561 ...  524  846 1357]\n",
      "[1071  986  228 ...  873  446  938]\n",
      "[ 596  899  823 ...  213 1394  939]\n",
      "[ 679  722  205 ... 1204  940 1436]\n",
      "[1139  827 1220 ... 1428 1195 1015]\n",
      "[1076 1015  178 ...  924  872  942]\n",
      "[1015 1226  467 ... 1161 1060 1423]\n",
      "[ 803 1228 1313 ...  627 1090  116]\n",
      "[1010  610  620 ...  990 1155  945]\n",
      "[545 482 803 ... 944 368 102]\n",
      "[ 891  855 1074 ... 1304  896  947]\n",
      "[1347  413  432 ...  294  481  948]\n",
      "[1345   47  723 ...  180 1171  949]\n",
      "[  78 1102  467 ...  516 1253  950]\n",
      "[ 803 1014 1100 ...  648  730  951]\n",
      "[ 616 1389  482 ... 1162 1118  952]\n",
      "[ 949  101  734 ...  953 1266  683]\n",
      "[ 432  659   77 ... 1199   95  954]\n",
      "[ 803  986 1391 ... 1250  873  955]\n",
      "[ 735 1071  663 ...  561  933  956]\n",
      "[1142   33  992 ...    4  361  116]\n",
      "[ 679 1288  554 ...  593   47  958]\n",
      "[ 318  525  432 ...  438 1342 1204]\n",
      "[ 611 1080  389 ... 1258  155  175]\n",
      "[ 452  885 1179 ...  747  889  961]\n",
      "[1242  311  403 ...  846  165  962]\n",
      "[1078  690  339 ... 1218 1348  963]\n",
      "[1185  998  855 ...  964  979 1349]\n",
      "[ 855 1241  555 ...  777 1162  965]\n",
      "[1148   59  515 ...  966  548  925]\n",
      "[ 397  167 1155 ... 1321  368  102]\n",
      "[325 318 453 ... 790 370 968]\n",
      "[308 178  84 ... 969 334 629]\n",
      "[ 307  489  343 ...  852 1417  970]\n",
      "[1086  264  253 ...  393  648  878]\n",
      "[1323 1241  531 ...  799  730  673]\n",
      "[ 476 1431 1316 ...  849 1090  572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1081  600 1086 ... 1338  747  974]\n",
      "[ 112  612   28 ...  401  672 1096]\n",
      "[ 392 1078 1241 ... 1309  767  620]\n",
      "[ 555   84  554 ... 1363 1161  977]\n",
      "[ 780    7 1132 ...  682  616  978]\n",
      "[1409  404  490 ...  737  991  979]\n",
      "[ 485  926  127 ... 1328 1398  980]\n",
      "[ 308 1276  525 ...  346 1279 1161]\n",
      "[ 711 1185  739 ...  874 1253  982]\n",
      "[489 890 308 ... 249 960 645]\n",
      "[1182  898  234 ... 1426  920  984]\n",
      "[ 593  914 1126 ...  746  409 1138]\n",
      "[ 874  300  106 ... 1427  253  986]\n",
      "[ 285   76  186 ...    4  574 1117]\n",
      "[ 199  320 1161 ...  244  790  988]\n",
      "[  69 1399 1241 ...  675 1162  989]\n",
      "[ 947  172  839 ... 1057  715  990]\n",
      "[ 856  162  633 ... 1204  281  991]\n",
      "[ 761 1324  471 ...  229  992  332]\n",
      "[ 432 1347  143 ... 1188  650  993]\n",
      "[1114 1428  799 ...   52  447  994]\n",
      "[ 530 1293  611 ...  252 1115  995]\n",
      "[1167  784  439 ... 1358  244 1112]\n",
      "[245 195 541 ... 229  86 997]\n",
      "[ 563 1348  755 ...  827  998 1071]\n",
      "[ 856  222 1093 ...  510  916  999]\n",
      "[  34  961 1013 ...  568  251 1000]\n",
      "[ 563  489 1139 ...  200 1249  274]\n",
      "[ 529  833 1075 ...  901 1016 1002]\n",
      "[1155 1352  366 ...  944  286 1003]\n",
      "[562 917 461 ... 332 910 220]\n",
      "[1272 1280  156 ... 1369  861  457]\n",
      "[1241 1347  222 ...  697 1281 1006]\n",
      "[ 716 1157   16 ...  572 1090  672]\n",
      "[ 180 1275 1029 ...  256  623  116]\n",
      "[ 748 1282 1182 ... 1051 1342 1009]\n",
      "[ 611  121  555 ...  500 1237 1162]\n",
      "[ 262 1323 1074 ...  582  276  481]\n",
      "[ 660  780    7 ...   14  841 1012]\n",
      "[1071  211 1046 ... 1275 1199 1016]\n",
      "[ 330  951 1126 ... 1170 1394  800]\n",
      "[ 667  218  615 ... 1128   42 1015]\n",
      "[1323 1086  525 ...  189 1002 1016]\n",
      "[1026  569 1258 ...  860  628 1017]\n",
      "[ 792 1236  335 ...  353 1328 1018]\n",
      "[  16  693  473 ...  348 1087 1019]\n",
      "[1280  369  432 ...  853 1405 1118]\n",
      "[ 615 1232  662 ...  252  674   94]\n",
      "[1228  186  712 ...  683  572 1096]\n",
      "[  17  678  441 ...  378 1037 1023]\n",
      "[ 998 1164 1071 ...  561 1024  448]\n",
      "[1347  291 1126 ...  772  896 1025]\n",
      "[1310  270  954 ...  149  404 1026]\n",
      "[ 476 1326  921 ...  575 1054 1027]\n",
      "[1314  750  419 ...  860  991 1028]\n",
      "[ 445   78  783 ...  896 1289 1029]\n",
      "[ 432 1323  547 ... 1132  644 1030]\n",
      "[ 178   17  635 ...  327   83 1031]\n",
      "[ 902 1077  357 ...  824  701 1312]\n",
      "[ 392   25  432 ... 1033  100  651]\n",
      "[ 616  299  339 ... 1245 1232 1034]\n",
      "[  65  429 1058 ...  324  870  370]\n",
      "[ 128 1154  748 ...  576  354  887]\n",
      "[1100   82  600 ... 1016   41  767]\n",
      "[ 978  476   56 ... 1398 1038  353]\n",
      "[ 841 1012  140 ... 1090  116  572]\n",
      "[1382  932 1347 ... 1342  426 1040]\n",
      "[ 970  439 1221 ... 1317  854  662]\n",
      "[141 645 534 ... 929 738  54]\n",
      "[1113  917  562 ...  232 1157 1233]\n",
      "[ 611 1343  178 ...  948  299  200]\n",
      "[651 482 310 ... 975 944 572]\n",
      "[1300  181  340 ...   70  777  346]\n",
      "[ 856  725  308 ...  718 1391 1047]\n",
      "[1139 1007  549 ... 1199 1238 1048]\n",
      "[1421 1080  722 ...  940 1049 1220]\n",
      "[ 432  603 1286 ...   19 1199 1050]\n",
      "[1293  748 1193 ...  991 1009 1051]\n",
      "[1071  587  570 ...  804 1253 1387]\n",
      "[ 376  206  277 ... 1199 1053 1016]\n",
      "[ 646 1201  616 ... 1380 1054  570]\n",
      "[ 222  920  906 ... 1150 1393 1055]\n",
      "[ 262  611  560 ...   95 1056  794]\n",
      "[1323  486   49 ... 1369 1057 1376]\n",
      "[1324  934  194 ...  772  232 1058]\n",
      "[1080   16  421 ...  680 1376  624]\n",
      "[ 221   84  762 ...   23 1060  202]\n",
      "[1420  432  262 ...  215  896  426]\n",
      "[1100  862  339 ...  851 1062  797]\n",
      "[ 129  710  858 ...   97 1394  213]\n",
      "[1112 1314  192 ... 1064  283 1246]\n",
      "[ 974  254  537 ...   91  981 1065]\n",
      "[1096   84 1287 ... 1393 1298 1066]\n",
      "[ 931 1186  902 ... 1383  934 1067]\n",
      "[ 978 1026   64 ...  582   47 1068]\n",
      "[1285 1383 1251 ...   33  146 1069]\n",
      "[ 320 1129   34 ...  197 1358  169]\n",
      "[ 456  345   34 ...  989 1071  998]\n",
      "[ 322  796  218 ...  852  681 1062]\n",
      "[1226  512  195 ... 1364   21   54]\n",
      "[1233  611 1111 ...  301  662  574]\n",
      "[ 856  222 1241 ...  675  661 1075]\n",
      "[ 847  597  480 ...  930  854 1076]\n",
      "[ 105  723   48 ...  276 1077  902]\n",
      "[ 544  954   95 ... 1418  101 1078]\n",
      "[  74  994  392 ... 1421  219 1079]\n",
      "[ 226  566 1010 ...  926  824 1142]\n",
      "[ 725  237 1409 ...  543 1081 1098]\n",
      "[ 650   47   57 ...  872 1411 1082]\n",
      "[ 429  195  989 ... 1002  255 1083]\n",
      "[1086   74 1340 ... 1204 1342 1084]\n",
      "[  90  636  277 ...  846 1357  524]\n",
      "[ 367 1201  925 ...  249 1363 1034]\n",
      "[1086  552  611 ...  348  851 1087]\n",
      "[ 308  432  302 ...  213 1088  163]\n",
      "[ 406  810  362 ... 1072  267 1089]\n",
      "[ 545  308    7 ...  116  572 1090]\n",
      "[ 929 1274  994 ...  346  282 1091]\n",
      "[  84  509  413 ...  106  629 1092]\n",
      "[ 112 1279 1152 ...  324 1222 1093]\n",
      "[ 463 1074  168 ...  534  746 1094]\n",
      "[1241   69  611 ...  677  252  900]\n",
      "[ 653 1157   16 ...  919  116 1090]\n",
      "[ 949  482  244 ...   26   91 1097]\n",
      "[ 174  748  714 ...  804  865 1098]\n",
      "[ 428 1174  945 ...  674  677 1099]\n",
      "[ 616  951 1026 ...  916  641 1100]\n",
      "[1086  978  998 ...  942  872 1101]\n",
      "[1230  369   36 ...  565  862 1102]\n",
      "[1106  989 1071 ... 1342 1304 1103]\n",
      "[ 890  792  581 ...  760  791 1104]\n",
      "[1417   90  122 ...  368 1413  535]\n",
      "[1053  121  521 ...  373  722 1106]\n",
      "[ 260  270 1412 ...  167  197  244]\n",
      "[ 994  456  318 ... 1047 1391 1108]\n",
      "[ 491 1080 1323 ...  229 1092  729]\n",
      "[ 685   84  587 ...  657  605 1245]\n",
      "[1232  921   86 ...  365  477 1111]\n",
      "[ 292  156  172 ...  322 1358 1112]\n",
      "[1185 1419   77 ... 1296  591 1113]\n",
      "[516 233 982 ... 691  51 102]\n",
      "[ 611 1293  679 ...   28  426 1115]\n",
      "[545 974 840 ... 711  94 102]\n",
      "[548 974 747 ... 572 849 116]\n",
      "[ 663  877  698 ...  777 1405 1118]\n",
      "[ 645 1014  628 ...  722  368  185]\n",
      "[ 233  803  994 ...  306 1120  517]\n",
      "[ 843  720  304 ... 1092 1367 1121]\n",
      "[1323  732 1073 ... 1064 1160 1122]\n",
      "[ 143 1074  706 ... 1387 1253 1123]\n",
      "[1352  418 1129 ...  916 1123  556]\n",
      "[ 406  864  802 ... 1125 1367 1121]\n",
      "[ 640  653  322 ...  906  963 1218]\n",
      "[ 555  198  615 ... 1234  398 1127]\n",
      "[ 560  392  402 ... 1242 1128   42]\n",
      "[ 610 1187  412 ...   34 1242 1129]\n",
      "[ 413  847  608 ... 1130  629  579]\n",
      "[ 615  563 1241 ...  888  508 1131]\n",
      "[ 829 1071  414 ...  644   86  921]\n",
      "[ 968  679  491 ...  383  697 1198]\n",
      "[ 467  237   18 ... 1199   19 1050]\n",
      "[ 369 1107  545 ...  334   94 1135]\n",
      "[ 392 1076  423 ...  642 1242 1136]\n",
      "[ 376 1211  854 ...  641  271 1137]\n",
      "[ 237  195  513 ...  377   60 1138]\n",
      "[1371   67  244 ...  901 1356 1139]\n",
      "[ 493 1135  887 ... 1215  485 1213]\n",
      "[1302 1375 1360 ...  821 1127  219]\n",
      "[ 390  808  626 ... 1233  782 1142]\n",
      "[1094   18 1349 ... 1009  917  461]\n",
      "[ 535    1   69 ...  147 1174 1144]\n",
      "[1105 1274  475 ... 1069  213 1145]\n",
      "[ 525  292 1348 ...  675 1349 1146]\n",
      "[ 486   49  578 ...  524 1090  116]\n",
      "[1365  431  564 ...  532  361  944]\n",
      "[ 656   89 1253 ...  846 1085 1357]\n",
      "[ 222  693  432 ...  959 1428 1150]\n",
      "[ 180  695  167 ...  944 1321  102]\n",
      "[1316  461   90 ... 1152 1121  611]\n",
      "[ 953  783  451 ...  939 1342 1394]\n",
      "[ 432  855 1268 ... 1056 1395   86]\n",
      "[ 178  204 1067 ...  745    8 1155]\n",
      "[ 800 1094 1157 ...  256  919 1090]\n",
      "[ 528  622  763 ... 1164 1349 1157]\n",
      "[  84 1140  325 ...  678  700 1158]\n",
      "[  50  156  935 ...  101 1159 1292]\n",
      "[1323  901  615 ... 1327  752 1160]\n",
      "[1071  998  989 ...  804 1423 1161]\n",
      "[1241 1399  516 ...  718 1391 1162]\n",
      "[1300  141 1276 ...  246  719 1163]\n",
      "[ 597 1032 1276 ...  706  139 1164]\n",
      "[ 590  571  514 ...  418  151 1165]\n",
      "[1084 1124  456 ...  236 1166  683]\n",
      "[ 926  815  824 ... 1062  394  681]\n",
      "[1276  994 1323 ...  332  674 1168]\n",
      "[1409  974  855 ...  854  438  950]\n",
      "[ 364  497  774 ...  294  800 1170]\n",
      "[ 222 1076 1375 ... 1187 1171 1104]\n",
      "[ 476  921  688 ...  849  944 1090]\n",
      "[ 929  961 1194 ...  412  220 1173]\n",
      "[1209  735 1143 ... 1189 1144 1174]\n",
      "[ 665 1310  597 ...  339  309 1175]\n",
      "[ 413  998  143 ...  874  294 1176]\n",
      "[ 375  398  875 ... 1002  982 1177]\n",
      "[  33 1261 1417 ... 1266 1178  683]\n",
      "[  80  274  782 ... 1376  853    0]\n",
      "[  89  653 1300 ...  401 1090  116]\n",
      "[1170  244  790 ...  719  428 1181]\n",
      "[1002   37  539 ...  743  853  443]\n",
      "[ 241  635  531 ... 1209 1183 1218]\n",
      "[1375  432  797 ... 1420 1184  668]\n",
      "[  33  218  822 ...  944  124 1090]\n",
      "[ 259 1108  671 ...   89  582 1186]\n",
      "[ 416  555 1274 ...  777 1091 1187]\n",
      "[ 989  826  318 ... 1233 1394 1188]\n",
      "[ 340  890 1227 ...  366 1174 1189]\n",
      "[ 339  750  978 ...  721   98 1190]\n",
      "[ 702 1106  468 ... 1215  485 1191]\n",
      "[ 843 1269   98 ... 1413 1307 1192]\n",
      "[ 949  847  473 ...  680  948 1367]\n",
      "[1359  289   99 ...  790   50 1194]\n",
      "[ 337   69  660 ...  683 1085 1357]\n",
      "[ 413  611 1276 ...  629  481  948]\n",
      "[1323  222  710 ...  641  449  271]\n",
      "[128 204 514 ... 822 447 123]\n",
      "[1080   93 1148 ...   41 1199 1016]\n",
      "[1238  188  538 ...  944  623  216]\n",
      "[  57  767 1245 ... 1365  786 1201]\n",
      "[ 476 1053  949 ... 1090  944  116]\n",
      "[ 799  432  710 ...  354  688 1203]\n",
      "[ 432  710  792 ...  438 1342 1204]\n",
      "[ 307 1371  790 ...  550   24 1205]\n",
      "[  84  466  525 ...  377  403 1206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 856 1323  491 ... 1206 1207  624]\n",
      "[1187  346  218 ...  459  572  574]\n",
      "[1069   60  403 ... 1392 1218 1209]\n",
      "[ 635  171 1300 ...  499  839  468]\n",
      "[1403  491  941 ... 1069 1211  186]\n",
      "[1278 1186  512 ... 1082  821 1212]\n",
      "[ 493  114  508 ...  462 1140 1213]\n",
      "[1094  949  529 ...  102 1151  368]\n",
      "[ 693  458  473 ...  485  574 1215]\n",
      "[1108  833  363 ...  409  385 1216]\n",
      "[ 172   45  755 ... 1220  697 1217]\n",
      "[ 917  833  749 ...  906  931 1218]\n",
      "[1187  152  777 ...  390 1219  913]\n",
      "[1116  191 1106 ...  854 1049 1220]\n",
      "[1102  563 1037 ...   90   61 1221]\n",
      "[1268  915  546 ...  370 1222 1013]\n",
      "[ 584  603 1322 ...  641  107 1223]\n",
      "[ 141  245 1084 ... 1224  861  850]\n",
      "[ 421  974 1274 ... 1367  624 1225]\n",
      "[ 310  521  628 ... 1054  779 1231]\n",
      "[  70 1422  162 ... 1353  991 1227]\n",
      "[ 257 1126 1422 ...  865   22  825]\n",
      "[ 429 1323 1421 ... 1261  533 1229]\n",
      "[1224  253  244 ... 1230  819  991]\n",
      "[ 310  245  629 ...  683 1231 1266]\n",
      "[1428 1281 1406 ...  281   57 1232]\n",
      "[ 318  989  432 ... 1188 1342 1233]\n",
      "[ 308 1276  564 ...  380 1379 1387]\n",
      "[  82  934  603 ... 1436  470 1235]\n",
      "[ 748 1209 1074 ... 1009  557 1236]\n",
      "[ 685    9  549 ...  948  500 1237]\n",
      "[ 682  826 1139 ...  324 1013 1238]\n",
      "[ 791  803  856 ...    1  162 1239]\n",
      "[1343  748  587 ...  481 1161 1363]\n",
      "[ 735  622  200 ...  662 1241  981]\n",
      "[ 565  125  610 ... 1242    2  345]\n",
      "[ 128  866  195 ...  950  907 1098]\n",
      "[1012  529  841 ... 1341  962   37]\n",
      "[ 803  432  732 ...   90  172 1245]\n",
      "[ 748  901  773 ...  470 1246  772]\n",
      "[1031  554  287 ... 1247  981  867]\n",
      "[1313 1100  182 ...  536  896 1204]\n",
      "[ 682  457  925 ... 1124  650 1249]\n",
      "[ 803   85  406 ...  830  955 1250]\n",
      "[ 506  523 1269 ...  980 1398  249]\n",
      "[615 422 660 ... 536 707 541]\n",
      "[ 750  855  308 ...  674 1188 1387]\n",
      "[ 120   72  695 ...  116 1254 1090]\n",
      "[ 570  143   31 ... 1124  526 1255]\n",
      "[ 367  318  195 ...  529 1304 1256]\n",
      "[ 194  679  773 ... 1204  229 1198]\n",
      "[ 865  190  552 ...  569 1258  935]\n",
      "[ 308 1155 1142 ...  631  252   94]\n",
      "[1034 1435  178 ... 1048  622 1260]\n",
      "[ 525 1421  890 ...  857  147 1261]\n",
      "[1157  406    7 ...  919 1090 1096]\n",
      "[1227  929  890 ...  800  213 1394]\n",
      "[1238  308  457 ...  916   54 1264]\n",
      "[1076 1086 1139 ... 1265    2 1242]\n",
      "[ 578  538  158 ...  953  683 1266]\n",
      "[ 564 1141  545 ...  177  368  967]\n",
      "[ 404  841  491 ...  162 1239   32]\n",
      "[ 122 1328  416 ... 1299 1016 1269]\n",
      "[1284  106  461 ... 1313  504 1270]\n",
      "[ 167  653 1371 ...  532  301  944]\n",
      "[ 891  430 1116 ...  772 1174  218]\n",
      "[ 506  563  525 ... 1273  642 1406]\n",
      "[1312  952  124 ...  345   34 1274]\n",
      "[ 459  711 1384 ... 1016 1002 1275]\n",
      "[ 489  581  963 ... 1204  438 1342]\n",
      "[ 506  784 1080 ... 1342  409 1277]\n",
      "[ 471  748  298 ...  438 1342 1278]\n",
      "[ 735  292 1353 ...  446 1394 1279]\n",
      "[ 803 1177 1311 ...  853  955  673]\n",
      "[1348   57 1230 ...  675 1281 1146]\n",
      "[1229  931  439 ...  926  824 1406]\n",
      "[ 694 1428  719 ... 1297   73 1283]\n",
      "[ 660  322   31 ... 1284 1218  906]\n",
      "[ 431  308 1009 ...  624  995 1393]\n",
      "[ 501  516 1033 ...  586  572 1286]\n",
      "[1221  120  482 ... 1090  849  116]\n",
      "[ 866 1074  608 ...  887  106 1288]\n",
      "[ 237  723  914 ... 1342 1436 1289]\n",
      "[1194  929 1352 ...  286 1290  449]\n",
      "[ 736  661 1292 ... 1397 1254 1271]\n",
      "[ 679  544   64 ... 1101  101 1292]\n",
      "[ 738 1362 1329 ...  190  517  306]\n",
      "[ 567   37  406 ... 1055 1146  828]\n",
      "[ 596 1071  735 ...  699  609 1295]\n",
      "[ 473 1399  363 ... 1362  170 1296]\n",
      "[ 322  774 1358 ... 1133   73 1283]\n",
      "[1352  432 1347 ...  305 1066 1298]\n",
      "[ 416  517 1192 ... 1372 1407 1299]\n",
      "[ 711  183  291 ...  871 1275 1300]\n",
      "[ 653  545  447 ...  368 1151  102]\n",
      "[ 879  325  525 ...  512 1302  721]\n",
      "[1367 1076  440 ... 1215 1303  883]\n",
      "[1106  989  941 ...  896  947 1304]\n",
      "[ 825  172 1203 ...   98  609 1305]\n",
      "[ 679  123  887 ...  944 1089  704]\n",
      "[ 133  798  554 ...  100  873 1307]\n",
      "[ 518  461  106 ... 1411 1082 1308]\n",
      "[1365  530  369 ... 1090  703  572]\n",
      "[ 624  383 1173 ... 1414  802   13]\n",
      "[ 262  432  920 ... 1012  995 1311]\n",
      "[ 945  468 1370 ...  182  701 1312]\n",
      "[ 998  525 1071 ... 1430 1313  612]\n",
      "[ 387  431   49 ... 1308  756 1314]\n",
      "[ 476  780  800 ...  944  823 1096]\n",
      "[ 678 1126  402 ...  418 1316  404]\n",
      "[ 867  112   33 ...  236  572 1317]\n",
      "[ 723  551  555 ...  947 1318 1138]\n",
      "[ 486  432  625 ... 1376  309 1319]\n",
      "[ 119  345  432 ...  339  680 1320]\n",
      "[ 670    7  732 ... 1090 1117  368]\n",
      "[ 562 1051  830 ... 1418  443  232]\n",
      "[1071  951 1141 ...  240  964  444]\n",
      "[ 491  635  535 ...  271  929 1324]\n",
      "[1076 1129  304 ... 1391  826 1325]\n",
      "[ 994 1107  362 ...   83  470 1326]\n",
      "[ 178  308 1276 ... 1121 1393 1367]\n",
      "[ 300  152  636 ... 1398 1018 1328]\n",
      "[  70 1073   31 ... 1296 1362 1329]\n",
      "[ 816 1211  978 ...  181  522 1330]\n",
      "[ 156  172 1353 ... 1358  244 1112]\n",
      "[ 564 1276  178 ... 1099   94  674]\n",
      "[1208  693  890 ...   91 1097 1333]\n",
      "[1052  838  578 ...  497  714 1334]\n",
      "[ 832 1365   40 ...  258 1160 1175]\n",
      "[1211  304 1228 ... 1284 1348 1336]\n",
      "[1235 1310  784 ...  815 1146 1337]\n",
      "[ 645  362  686 ...  974  889 1338]\n",
      "[ 750  602 1055 ... 1050  964 1339]\n",
      "[1241 1113 1235 ... 1146  622 1340]\n",
      "[ 491  435  460 ...  198   37 1341]\n",
      "[1071 1172  855 ...  438  433 1342]\n",
      "[ 257  506  744 ...  365  210 1343]\n",
      "[ 925 1192 1201 ...   19 1344  923]\n",
      "[ 711 1106  178 ... 1436  940  149]\n",
      "[ 546  915 1167 ... 1371  614 1346]\n",
      "[1276  245  327 ...  381  143  246]\n",
      "[ 186 1211   62 ... 1336   57 1348]\n",
      "[ 185 1108  259 ...  621  721 1349]\n",
      "[1108  616  978 ...  181  721 1350]\n",
      "[ 803 1241  640 ...  204 1239 1351]\n",
      "[ 449  439  862 ... 1042  414 1352]\n",
      "[ 103  590  234 ... 1296  889 1353]\n",
      "[ 819  485 1238 ...  216  227  944]\n",
      "[1129  820  630 ...  770  100  668]\n",
      "[ 257 1164  244 ...  901 1139 1356]\n",
      "[1079 1253 1170 ... 1085  846 1357]\n",
      "[ 855 1246  710 ...  988  244 1358]\n",
      "[ 458  486  879 ...  443  678 1367]\n",
      "[ 525 1287  951 ...  950  502   26]\n",
      "[ 188  734 1371 ...  852  511 1361]\n",
      "[1021   84 1208 ... 1296 1329 1362]\n",
      "[  65 1352  587 ... 1188  860 1363]\n",
      "[ 178 1310  615 ...  556  210 1203]\n",
      "[ 465  711  596 ...  644  431 1365]\n",
      "[1272  339  486 ... 1366 1204 1436]\n",
      "[ 656 1431  522 ...  624  410 1367]\n",
      "[ 180  188 1289 ...  919  116  256]\n",
      "[1067  207 1409 ...  861 1057 1369]\n",
      "[ 195 1149  666 ...  916  582 1370]\n",
      "[ 914  954   95 ...  642  362 1371]\n",
      "[ 744 1102 1227 ...  695  263 1372]\n",
      "[1352   65 1209 ...  593 1290  449]\n",
      "[ 410  335  392 ...  511 1374  827]\n",
      "[1346  742  335 ...  555  665 1375]\n",
      "[ 685 1332 1249 ...  309 1174 1376]\n",
      "[ 962  167  447 ...  574  934 1377]\n",
      "[ 207  832  260 ...  131  353 1378]\n",
      "[ 653  611  810 ...  373  284 1379]\n",
      "[1220 1049  854 ...  570  815 1380]\n",
      "[ 329   24   60 ... 1156   68  672]\n",
      "[ 476  712 1341 ...  124  256  116]\n",
      "[ 428   31  628 ... 1067 1383  934]\n",
      "[1289  968 1399 ...  919  116 1090]\n",
      "[ 998 1071  748 ...  650 1050  921]\n",
      "[ 798 1242  974 ... 1367  822 1386]\n",
      "[ 431  653  599 ... 1387  605  924]\n",
      "[ 392  936   56 ... 1388 1254 1133]\n",
      "[1093 1126   69 ...  651 1175 1389]\n",
      "[ 737 1430  773 ... 1162  945  123]\n",
      "[1241  364  358 ... 1047 1162 1391]\n",
      "[ 369  302 1246 ...  609  693 1392]\n",
      "[ 679 1253  982 ...  512  896 1393]\n",
      "[ 989  998 1071 ...  721 1295 1394]\n",
      "[ 855  891  465 ...  699 1394 1395]\n",
      "[ 803  458  974 ...  137 1163 1396]\n",
      "[ 121 1353  889 ...  944  326 1358]\n",
      "[ 335  339  772 ... 1018  353 1398]\n",
      "[1030   82  723 ... 1389  197 1399]\n",
      "[ 921  255  871 ... 1054  227  944]\n",
      "[ 823  914 1039 ...  689 1159 1401]\n",
      "[1142  706 1152 ...  481  804 1402]\n",
      "[ 888  508  156 ...  380  863 1403]\n",
      "[1310 1015  984 ...  924  872 1404]\n",
      "[720 595 803 ... 777 886 853]\n",
      "[  31  149 1043 ...  597 1406  934]\n",
      "[1127  190  866 ...  189 1275 1407]\n",
      "[1230  545 1195 ... 1408  102 1054]\n",
      "[1073  304 1187 ...  511 1361 1409]\n",
      "[1323 1276  222 ...  629  562 1410]\n",
      "[ 690  178  439 ...  232  872 1411]\n",
      "[1227  122  308 ...  220  235  246]\n",
      "[  90  523 1375 ... 1405 1369 1413]\n",
      "[ 736   99  260 ... 1131  811 1414]\n",
      "[ 363  805  432 ...  896  426 1415]\n",
      "[ 297  283  461 ... 1195 1266 1357]\n",
      "[ 490 1374 1117 ... 1256  874 1417]\n",
      "[1062  525  341 ... 1292 1418  101]\n",
      "[ 538   28  491 ... 1266  535 1357]\n",
      "[ 156  574   64 ...  377 1420  668]\n",
      "[1087 1220 1049 ...  256  846 1421]\n",
      "[ 421  564  802 ...  675    1 1422]\n",
      "[ 554   65  555 ... 1423  874 1367]\n",
      "[ 855  748  360 ... 1161 1363 1424]\n",
      "[ 466  262 1340 ...  522  181 1425]\n",
      "[1182 1224  696 ... 1350  456 1426]\n",
      "[1011  222  152 ...   37  614 1427]\n",
      "[  16  693  262 ...  200 1393 1428]\n",
      "[ 615 1323  222 ...  912 1204 1429]\n",
      "[1353  917  554 ...  756 1349 1430]\n",
      "[ 157  978 1239 ...  656 1336 1431]\n",
      "[1276  523  803 ...   94  252 1432]\n",
      "[ 235  457 1073 ... 1264 1117 1433]\n",
      "[1248  564  538 ... 1434 1041  236]\n",
      "[1182  886 1055 ... 1275 1350 1435]\n",
      "[1071  205  671 ...  896  940 1436]\n",
      "[ 233    7 1170 ...  919  256  116]\n",
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "0.557\n",
    "```\n",
    "\n",
    "You managed to translate words from one language to another language\n",
    "without ever seing them with almost 56% accuracy by using some basic\n",
    "linear algebra and learning a mapping of words from one language to another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "# 3. LSH and document search\n",
    "\n",
    "In this part of the assignment, you will implement a more efficient version\n",
    "of k-nearest neighbors using locality sensitive hashing.\n",
    "You will then apply this to document search.\n",
    "\n",
    "* Process the tweets and represent each tweet as a vector (represent a\n",
    "document with a vector embedding).\n",
    "* Use locality sensitive hashing and k nearest neighbors to find tweets\n",
    "that are similar to a given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-1\"></a>\n",
    "\n",
    "### 3.1 Getting the document embeddings\n",
    "\n",
    "#### Bag-of-words (BOW) document models\n",
    "Text documents are sequences of words.\n",
    "* The ordering of words makes a difference. For example, sentences \"Apple pie is\n",
    "better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n",
    "have opposite meanings due to the word ordering.\n",
    "* However, for some applications, ignoring the order of words can allow\n",
    "us to train an efficient and still effective model.\n",
    "* This approach is called Bag-of-words document model.\n",
    "\n",
    "#### Document embeddings\n",
    "* Document embedding is created by summing up the embeddings of all words\n",
    "in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-07\"></a>\n",
    "\n",
    "**Exercise 07**:\n",
    "Complete the `get_document_embedding()` function.\n",
    "* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n",
    "* It takes in a docoument (as a string) and a dictionary, `en_embeddings`\n",
    "* It processes the document, and looks up the corresponding embedding of each word.\n",
    "* It then sums them up and returns the sum of all word vectors of that processed tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> You can handle missing words easier by using the `get()` method of the python dictionary instead of the bracket notation (i.e. \"[ ]\"). See more about it <a href=\"https://stackoverflow.com/a/11041421/12816433\" >here</a> </li>\n",
    "    <li> The default value for missing word should be the zero vector. Numpy will <a href=\"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" > broadcast </a> simple 0 scalar into a vector of zeros during the summation.</li>\n",
    "    <li>Alternatively, skip the addition if a word is not in the dictonary. </li>\n",
    "    <li>  You can use your `process_tweet()` function which allows you to process the tweet. The function just takes in a tweet and returns a list of words.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C12 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_embedding(tweet, en_embeddings): \n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding += en_embeddings.get(word, 0)\n",
    "    ### END CODE HERE ###\n",
    "    return doc_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNQ_C13 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# testing your function\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "```\n",
    "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-08\"></a>\n",
    "\n",
    "### Exercise 08\n",
    "\n",
    "#### Store all document vectors into a dictionary\n",
    "Now, let's store all the tweet embeddings into a dictionary.\n",
    "Implement `get_document_vecs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C14 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C15 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "```\n",
    "length of dictionary 10000\n",
    "shape of document_vecs (10000, 300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-2\"></a>\n",
    "\n",
    "## 3.2 Looking up the tweets\n",
    "\n",
    "Now you have a vector of dimension (m,d) where `m` is the number of tweets\n",
    "(10,000) and `d` is the dimension of the embeddings (300).  Now you\n",
    "will input a tweet, and use cosine similarity to see which tweet in our\n",
    "corpus is similar to your tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# this gives you a similar tweet as your input.\n",
    "# this implementation is vectorized...\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```\n",
    "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-3\"></a>\n",
    "\n",
    "## 3.3 Finding the most similar tweets with LSH\n",
    "\n",
    "You will now implement locality sensitive hashing (LSH) to identify the most similar tweet.\n",
    "* Instead of looking at all 10,000 vectors, you can just search a subset to find\n",
    "its nearest neighbors.\n",
    "\n",
    "Let's say your data points are plotted like this:\n",
    "\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='one.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 3 </div>\n",
    "\n",
    "You can divide the vector space into regions and search within one region for nearest neighbors of a given vector.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='four.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 4 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ vectors.\n",
    "* For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-4\"></a>\n",
    "\n",
    "## 3.4 Getting the hash number for a vector\n",
    "\n",
    "For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n",
    "\n",
    "### Hyperlanes in vector spaces\n",
    "* In $3$-dimensional vector space, the hyperplane is a regular plane. In $2$ dimensional vector space, the hyperplane is a line.\n",
    "* Generally, the hyperplane is subspace which has dimension $1$ lower than the original vector space has.\n",
    "* A hyperplane is uniquely defined by its normal vector.\n",
    "* Normal vector $n$ of the plane $\\pi$ is the vector to which all vectors in the plane $\\pi$ are orthogonal (perpendicular in $3$ dimensional case).\n",
    "\n",
    "### Using Hyperplanes to split the vector space\n",
    "We can use a hyperplane to split the vector space into $2$ parts.\n",
    "* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
    "* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
    "\n",
    "### Encoding hash buckets\n",
    "* For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n",
    "* When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n",
    "* Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n",
    "* If you calculate the dot product with each plane in the same order for every vector, you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-09\"></a>\n",
    "\n",
    "### Exercise 09: Implementing hash buckets\n",
    "\n",
    "We've initialized hash table `hashes` for you. It is list of `N_UNIVERSES` matrices, each describes its own hash table. Each matrix has `N_DIMS` rows and `N_PLANES` columns. Every column of that matrix is a `N_DIMS`-dimensional normal vector for each of `N_PLANES` hyperplanes which are used for creating buckets of the particular hash table.\n",
    "\n",
    "*Exercise*: Your task is to complete the function `hash_value_of_vector` which places vector `v` in the correct hash bucket.\n",
    "\n",
    "* First multiply your vector `v`, with a corresponding plane. This will give you a vector of dimension $(1,\\text{N_planes})$.\n",
    "* You will then convert every element in that vector to 0 or 1.\n",
    "* You create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n",
    "* You then compute the unique number for the vector by iterating over `N_PLANES`\n",
    "* Then you multiply $2^i$ times the corresponding bit (0 or 1).\n",
    "* You will then store that sum in the variable `hash_value`.\n",
    "\n",
    "**Intructions:** Create a hash for the vector in the function below.\n",
    "Use this formula:\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the sets of planes\n",
    "* Create multiple (25) sets of planes (the planes that divide up the region).\n",
    "* You can think of these as 25 separate ways of dividing up the vector space with a different set of planes.\n",
    "* Each element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 10 columns (there are 10 planes in each \"universe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> numpy.squeeze() removes unused dimensions from an array; for instance, it converts a (10,1) 2D array into a (10,) 1D array</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    dot_product = np.dot(v, planes)\n",
    "\n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    h = sign_of_dot_product>=0\n",
    "\n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += 2**i *(h[i])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```\n",
    "The hash value for this vector, and the set of planes at index 0, is 768\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-5\"></a>\n",
    "\n",
    "## 3.5 Creating a hash table\n",
    "\n",
    "<a name=\"ex-10\"></a>\n",
    "\n",
    "### Exercise 10\n",
    "\n",
    "Given that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='table.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px;\" />  </div>\n",
    "\n",
    "We have given you the `make_hash_table` function, which maps the tweet vectors to a bucket and stores the vector there. It returns the `hash_table` and the `id_table`. The `id_table` allows you know which vector in a certain bucket corresponds to what tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> a dictionary comprehension, similar to a list comprehension, looks like this: `{i:0 for i in range(10)}`, where the key is 'i' and the value is zero for all key-value pairs. </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# This is the code used to create a hash table: feel free to read over it\n",
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2**(num_of_planes)\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return hash_table, id_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 3 document vectors\n",
      "The id table at key 0 has 3\n",
      "The first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "np.random.seed(0)\n",
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "```\n",
    "The hash table at key 0 has 3 document vectors\n",
    "The id table at key 0 has 3\n",
    "The first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-6\"></a>\n",
    "\n",
    "### 3.6 Creating all hash tables\n",
    "\n",
    "You can now hash your vectors and store them in a hash table that\n",
    "would allow you to quickly look up and search for similar vectors.\n",
    "Run the cell below to create the hashes. By doing so, you end up having\n",
    "several tables which have all the vectors. Given a vector, you then\n",
    "identify the buckets in all the tables.  You can then iterate over the\n",
    "buckets and consider much fewer vectors. The more buckets you use, the\n",
    "more accurate your lookup will be, but also the longer it will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate K-NN\n",
    "\n",
    "<a name=\"ex-11\"></a>\n",
    "\n",
    "### Exercise 11\n",
    "\n",
    "Implement approximate K nearest neighbors using locality sensitive hashing,\n",
    "to search for documents that are similar to a given document at the\n",
    "index `doc_id`.\n",
    "\n",
    "##### Inputs\n",
    "* `doc_id` is the index into the document list `all_tweets`.\n",
    "* `v` is the document vector for the tweet in `all_tweets` at index `doc_id`.\n",
    "* `planes_l` is the list of planes (the global variable created earlier).\n",
    "* `k` is the number of nearest neighbors to search for.\n",
    "* `num_universes_to_use`: to save time, we can use fewer than the total\n",
    "number of available universes.  By default, it's set to `N_UNIVERSES`,\n",
    "which is $25$ for this assignment.\n",
    "\n",
    "The `approximate_knn` function finds a subset of candidate vectors that\n",
    "are in the same \"hash bucket\" as the input vector 'v'.  Then it performs\n",
    "the usual k-nearest neighbors search on this subset (instead of searching\n",
    "through all 10,000 tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> There are many dictionaries used in this function.  Try to print out planes_l, hash_tables, id_tables to understand how they are structured, what the keys represent, and what the values contain.</li>\n",
    "    <li> To remove an item from a list, use `.remove()` </li>\n",
    "    <li> To append to a list, use `.append()` </li>\n",
    "    <li> To add to a set, use `.add()` </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n",
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as possible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # remove the id of the document that we're searching\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                \n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 77 vecs\n",
      "[67 21 76 65 63 72 59 36 19 68 73 44 60  4 48 69 66 55 41 30 64 54 71 38\n",
      " 75 13 15 12 70 16 17 57 56 10 74 52 39 20 32 53 24 40 43 11 25 49 28 33\n",
      " 47 42 34 58 35 18 37 61 62 31 29  9  7  1  2  6  5  3 50 14 51 23 46 45\n",
      " 22 27 26  8  0]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Sample\n",
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 2140\n",
      "document contents: @PopsRamjet come one, every now and then is not so bad :)\n",
      "Nearest neighbor at document id 701\n",
      "document contents: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Conclusion\n",
    "Congratulations - Now you can look up vectors that are similar to the\n",
    "encoding of your tweet using LSH!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
